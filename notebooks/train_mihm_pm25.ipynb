{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mihm.data.process import multi_cat_to_one_hot, binary_to_one_hot, standardize_continuous_cols, convert_categorical_to_ordinal\n",
    "from mihm.data.trainutils import train_test_split\n",
    "from mihm.model.mihm import MIHM, IndexPredictionModel\n",
    "from mihm.model.mihm_dataset import MIHMDataset\n",
    "from mihm.model.modelutils import get_index_prediction_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_cols = [\"zPCPhenoAge_acc\", \"pm25_7\", \"age2016\", \"female\", \"racethn\", \"eduy\", \"ihs_wealthf2016\", \"pmono\", \"PNK_pct\", \n",
    "            \"PBcell_pct\", \"PCD8_Plus_pct\", \"PCD4_Plus_pct\", \"PNCD8_Plus_pct\",\n",
    "            \"smoke2016\", \"drink2016\", \"bmi2016\", \"tractdis\", \"urban\", \"mar_cat2\", \"psyche2016\", \"stroke2016\", \"hibpe2016\",\n",
    "            \"diabe2016\", \"hearte2016\", \"ltactx2016\", \"mdactx2016\", \"vgactx2016\", \"chd2016\", \"dep2016\", \"adl2016\", \n",
    "            \"living2016\", \"division\"]\n",
    "\n",
    "\n",
    "df = pd.read_stata('../HeatResilience.dta', columns=read_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['female', 'racethn', 'urban', 'mar_cat2', \"psyche2016\", \"stroke2016\", \n",
    "                    \"hibpe2016\", \"diabe2016\", \"hearte2016\",  'living2016', 'division',]\n",
    "ordinal_cols = [\"smoke2016\",  'drink2016', 'ltactx2016', 'mdactx2016', 'vgactx2016']\n",
    "continuous_cols = ['eduy', 'ihs_wealthf2016', 'age2016', 'pmono','bmi2016', 'tractdis', 'chd2016', 'dep2016', 'adl2016', \"pm25_7\",\n",
    "                    \"PNK_pct\", \"PBcell_pct\", \"PCD8_Plus_pct\", \"PCD4_Plus_pct\", \"PNCD8_Plus_pct\",]\n",
    "for c in categorical_cols:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "# categorical = [c for c in df.columns if df[c].dtype == \"category\"]\n",
    "# separate binary vs multicategory cols\n",
    "binary_cats = [c for c in categorical_cols if df[c].nunique() <=2]\n",
    "multi_cats = [c for c in categorical_cols if df[c].nunique() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess df for model\n",
    "df = binary_to_one_hot(df, binary_cats) # convert binary to one hot\n",
    "df = multi_cat_to_one_hot(df, multi_cats) # convert multi cat to one hot\n",
    "df = convert_categorical_to_ordinal(df, ordinal_cols) # convert ordinal to ordinal\n",
    "df_norm, mean_std_dict = standardize_continuous_cols(df, continuous_cols+ordinal_cols) # standardize continuous cols\n",
    "df_norm.dropna(inplace=True) # drop Nan rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zPCPhenoAge_acc', 'pm25_7', 'age2016', 'female', 'eduy',\n",
       "       'ihs_wealthf2016', 'pmono', 'PNK_pct', 'PBcell_pct', 'PCD8_Plus_pct',\n",
       "       'PCD4_Plus_pct', 'PNCD8_Plus_pct', 'smoke2016', 'drink2016', 'bmi2016',\n",
       "       'tractdis', 'mar_cat2', 'psyche2016', 'stroke2016', 'hibpe2016',\n",
       "       'diabe2016', 'hearte2016', 'ltactx2016', 'mdactx2016', 'vgactx2016',\n",
       "       'chd2016', 'dep2016', 'adl2016', 'living2016', 'racethn_0. NHW',\n",
       "       'racethn_1. NHB', 'racethn_2. Hispanic', 'racethn_3. Others',\n",
       "       'urban_1. urban', 'urban_2. suurban (code 2)', 'urban_3. ex-urban',\n",
       "       'division_Northeast', 'division_Midwest', 'division_South',\n",
       "       'division_West'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['female', 'eduy', 'ihs_wealthf2016', 'pmono', 'bmi2016', \"age2016\",\n",
    "            'tractdis', 'mar_cat2', 'psyche2016', 'stroke2016', 'hibpe2016', \n",
    "            'diabe2016', 'hearte2016', 'chd2016', 'dep2016', 'adl2016', 'living2016', \n",
    "            'smoke2016', 'drink2016', 'ltactx2016', 'mdactx2016', 'vgactx2016', # ordinals\n",
    "            'racethn_0. NHW', 'racethn_1. NHB', 'racethn_2. Hispanic', 'racethn_3. Others', # multi cats\n",
    "            'urban_1. urban', 'urban_2. suurban (code 2)', 'urban_3. ex-urban', \n",
    "            'division_Northeast', 'division_Midwest', 'division_South', 'division_West',\n",
    "            \"PNK_pct\", \"PBcell_pct\", \"PCD8_Plus_pct\", \"PCD4_Plus_pct\", \"PNCD8_Plus_pct\",]\n",
    "controlled_cols = [\n",
    "    \"pm25_7\",\n",
    "    \"pmono\",\n",
    "    \"PNK_pct\",\n",
    "    \"PBcell_pct\",\n",
    "    \"PCD8_Plus_pct\",\n",
    "    \"PCD4_Plus_pct\",\n",
    "    \"PNCD8_Plus_pct\",\n",
    "]\n",
    "interaction_predictors = [\n",
    "    \"female\", \"racethn_0. NHW\", \"racethn_1. NHB\", \"racethn_2. Hispanic\", \"racethn_3. Others\",\n",
    "    'eduy', 'ihs_wealthf2016', 'bmi2016', \n",
    "    'tractdis', 'mar_cat2', 'psyche2016', 'stroke2016', 'hibpe2016', \n",
    "    'diabe2016', 'hearte2016', 'chd2016', 'dep2016', 'adl2016', 'living2016', \n",
    "    'smoke2016', 'drink2016', 'ltactx2016', 'mdactx2016', 'vgactx2016',\n",
    "    'urban_1. urban', 'urban_2. suurban (code 2)', 'urban_3. ex-urban', \n",
    "    'division_Northeast', 'division_Midwest', 'division_South', 'division_West',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactor\n",
    "pm25_cont_np = df_norm[\"pm25_7\"].to_numpy()\n",
    "# controlled vars\n",
    "controlled_vars_np = df_norm[controlled_cols].to_numpy()\n",
    "# interaction input vars\n",
    "interaction_vars_np = df_norm[interaction_predictors].to_numpy()\n",
    "# dependent var (label)\n",
    "pheno_epi_np = df_norm[\"zPCPhenoAge_acc\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 3316\n"
     ]
    }
   ],
   "source": [
    "num_elems, _ = controlled_vars_np.shape\n",
    "print(\"number of data points: {}\".format(num_elems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train and test\n",
    "train_idx, test_idx = train_test_split(num_elems, 0.7)\n",
    "train_pm25_cont = pm25_cont_np[train_idx]\n",
    "train_controlled_vars = controlled_vars_np[train_idx]\n",
    "train_interaction_vars = interaction_vars_np[train_idx]\n",
    "train_pheno_epi = pheno_epi_np[train_idx]\n",
    "\n",
    "test_pm25_cont = torch.from_numpy(pm25_cont_np[test_idx].astype(np.float32))\n",
    "test_controlled_vars = torch.from_numpy(controlled_vars_np[test_idx].astype(np.float32))\n",
    "test_interaction_vars = torch.from_numpy(interaction_vars_np[test_idx].astype(np.float32))\n",
    "test_pheno_epi = torch.from_numpy(pheno_epi_np[test_idx].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "train_dataset = MIHMDataset(train_pm25_cont, train_controlled_vars, train_interaction_vars, train_pheno_epi)\n",
    "dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_var_size = interaction_vars_np.shape[1]\n",
    "controlled_var_size = controlled_vars_np.shape[1]\n",
    "hidden_layer_sizes = [50, 10, 1]\n",
    "model = MIHM(interaction_var_size, controlled_var_size, hidden_layer_sizes, include_interactor_bias=False, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mseLoss = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.1)\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namj/miniconda3/envs/hrs/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([995])) that is different to the input size (torch.Size([995, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss: 4.961288928985596\n",
      "Epoch 1/500 done!\n",
      "Training Loss: 8.606128414471945\n",
      "Testing Loss: 4.649364471435547\n",
      "Epoch 2/500 done!\n",
      "Training Loss: 7.953260838985443\n",
      "Testing Loss: 4.446926116943359\n",
      "Epoch 3/500 done!\n",
      "Training Loss: 8.099991083145142\n",
      "Testing Loss: 4.269570827484131\n",
      "Epoch 4/500 done!\n",
      "Training Loss: 7.664976418018341\n",
      "Testing Loss: 4.1307373046875\n",
      "Epoch 5/500 done!\n",
      "Training Loss: 6.405217965443929\n",
      "Testing Loss: 4.015005588531494\n",
      "Epoch 6/500 done!\n",
      "Training Loss: 5.941045840581258\n",
      "Testing Loss: 3.9171500205993652\n",
      "Epoch 7/500 done!\n",
      "Training Loss: 5.377009183168411\n",
      "Testing Loss: 3.8145899772644043\n",
      "Epoch 8/500 done!\n",
      "Training Loss: 5.1767205993334455\n",
      "Testing Loss: 3.7239327430725098\n",
      "Epoch 9/500 done!\n",
      "Training Loss: 4.918573011954625\n",
      "Testing Loss: 3.6355395317077637\n",
      "Epoch 10/500 done!\n",
      "Training Loss: 5.025175998608272\n",
      "Testing Loss: 3.5446622371673584\n",
      "Epoch 11/500 done!\n",
      "Training Loss: 4.543425053358078\n",
      "Testing Loss: 3.468491554260254\n",
      "Epoch 12/500 done!\n",
      "Training Loss: 4.409499377012253\n",
      "Testing Loss: 3.3954567909240723\n",
      "Epoch 13/500 done!\n",
      "Training Loss: 4.246680438518524\n",
      "Testing Loss: 3.328847646713257\n",
      "Epoch 14/500 done!\n",
      "Training Loss: 4.196933696667354\n",
      "Testing Loss: 3.2628138065338135\n",
      "Epoch 15/500 done!\n",
      "Training Loss: 4.721828073263168\n",
      "Testing Loss: 3.198484420776367\n",
      "Epoch 16/500 done!\n",
      "Training Loss: 3.82405424118042\n",
      "Testing Loss: 3.141646146774292\n",
      "Epoch 17/500 done!\n",
      "Training Loss: 3.7305201490720115\n",
      "Testing Loss: 3.082183599472046\n",
      "Epoch 18/500 done!\n",
      "Training Loss: 3.6526814103126526\n",
      "Testing Loss: 3.0260260105133057\n",
      "Epoch 19/500 done!\n",
      "Training Loss: 3.7049666146437326\n",
      "Testing Loss: 2.971231698989868\n",
      "Epoch 20/500 done!\n",
      "Training Loss: 4.021753147244453\n",
      "Testing Loss: 2.917602062225342\n",
      "Epoch 21/500 done!\n",
      "Training Loss: 3.454874853293101\n",
      "Testing Loss: 2.8645482063293457\n",
      "Epoch 22/500 done!\n",
      "Training Loss: 3.3808976312478385\n",
      "Testing Loss: 2.8138911724090576\n",
      "Epoch 23/500 done!\n",
      "Training Loss: 3.032774190107981\n",
      "Testing Loss: 2.7622358798980713\n",
      "Epoch 24/500 done!\n",
      "Training Loss: 3.082200517257055\n",
      "Testing Loss: 2.7147111892700195\n",
      "Epoch 25/500 done!\n",
      "Training Loss: 2.9134725431601205\n",
      "Testing Loss: 2.6682589054107666\n",
      "Epoch 26/500 done!\n",
      "Training Loss: 2.954333712657293\n",
      "Testing Loss: 2.6215782165527344\n",
      "Epoch 27/500 done!\n",
      "Training Loss: 2.81144588192304\n",
      "Testing Loss: 2.577820301055908\n",
      "Epoch 28/500 done!\n",
      "Training Loss: 2.7773940612872443\n",
      "Testing Loss: 2.535344362258911\n",
      "Epoch 29/500 done!\n",
      "Training Loss: 3.0393414398034415\n",
      "Testing Loss: 2.493088483810425\n",
      "Epoch 30/500 done!\n",
      "Training Loss: 2.7034093538920083\n",
      "Testing Loss: 2.4551031589508057\n",
      "Epoch 31/500 done!\n",
      "Training Loss: 2.752035597960154\n",
      "Testing Loss: 2.4160220623016357\n",
      "Epoch 32/500 done!\n",
      "Training Loss: 2.6743741234143577\n",
      "Testing Loss: 2.3785202503204346\n",
      "Epoch 33/500 done!\n",
      "Training Loss: 2.557333365082741\n",
      "Testing Loss: 2.341341495513916\n",
      "Epoch 34/500 done!\n",
      "Training Loss: 2.5509818146626153\n",
      "Testing Loss: 2.305734634399414\n",
      "Epoch 35/500 done!\n",
      "Training Loss: 2.5225464900334678\n",
      "Testing Loss: 2.2712831497192383\n",
      "Epoch 36/500 done!\n",
      "Training Loss: 2.435153449575106\n",
      "Testing Loss: 2.2370612621307373\n",
      "Epoch 37/500 done!\n",
      "Training Loss: 2.342483709255854\n",
      "Testing Loss: 2.2045767307281494\n",
      "Epoch 38/500 done!\n",
      "Training Loss: 2.3686955074469247\n",
      "Testing Loss: 2.1727044582366943\n",
      "Epoch 39/500 done!\n",
      "Training Loss: 2.305678447087606\n",
      "Testing Loss: 2.1418604850769043\n",
      "Epoch 40/500 done!\n",
      "Training Loss: 2.2708051254351935\n",
      "Testing Loss: 2.111295461654663\n",
      "Epoch 41/500 done!\n",
      "Training Loss: 2.152175714572271\n",
      "Testing Loss: 2.08217716217041\n",
      "Epoch 42/500 done!\n",
      "Training Loss: 2.1983284056186676\n",
      "Testing Loss: 2.0533523559570312\n",
      "Epoch 43/500 done!\n",
      "Training Loss: 2.1357393066088357\n",
      "Testing Loss: 2.024953603744507\n",
      "Epoch 44/500 done!\n",
      "Training Loss: 2.0500006725390754\n",
      "Testing Loss: 1.998098611831665\n",
      "Epoch 45/500 done!\n",
      "Training Loss: 2.109369079271952\n",
      "Testing Loss: 1.9715425968170166\n",
      "Epoch 46/500 done!\n",
      "Training Loss: 1.9619366923967998\n",
      "Testing Loss: 1.945875644683838\n",
      "Epoch 47/500 done!\n",
      "Training Loss: 2.0119344294071198\n",
      "Testing Loss: 1.92074716091156\n",
      "Epoch 48/500 done!\n",
      "Training Loss: 1.9637164672215779\n",
      "Testing Loss: 1.8954365253448486\n",
      "Epoch 49/500 done!\n",
      "Training Loss: 1.9820232391357422\n",
      "Testing Loss: 1.8718359470367432\n",
      "Epoch 50/500 done!\n",
      "Training Loss: 1.8668328076601028\n",
      "Testing Loss: 1.848630666732788\n",
      "Epoch 51/500 done!\n",
      "Training Loss: 1.8461938450733821\n",
      "Testing Loss: 1.8260319232940674\n",
      "Epoch 52/500 done!\n",
      "Training Loss: 1.9304811507463455\n",
      "Testing Loss: 1.8037419319152832\n",
      "Epoch 53/500 done!\n",
      "Training Loss: 1.8041372746229172\n",
      "Testing Loss: 1.7818872928619385\n",
      "Epoch 54/500 done!\n",
      "Training Loss: 1.73679514726003\n",
      "Testing Loss: 1.7623062133789062\n",
      "Epoch 55/500 done!\n",
      "Training Loss: 1.777542268236478\n",
      "Testing Loss: 1.741637110710144\n",
      "Epoch 56/500 done!\n",
      "Training Loss: 1.7082267279426258\n",
      "Testing Loss: 1.721254825592041\n",
      "Epoch 57/500 done!\n",
      "Training Loss: 1.7093542516231537\n",
      "Testing Loss: 1.7025140523910522\n",
      "Epoch 58/500 done!\n",
      "Training Loss: 1.667437766989072\n",
      "Testing Loss: 1.6832242012023926\n",
      "Epoch 59/500 done!\n",
      "Training Loss: 1.7512663354476292\n",
      "Testing Loss: 1.6643089056015015\n",
      "Epoch 60/500 done!\n",
      "Training Loss: 1.6689291795094807\n",
      "Testing Loss: 1.6450121402740479\n",
      "Epoch 61/500 done!\n",
      "Training Loss: 1.6423237373431523\n",
      "Testing Loss: 1.627535104751587\n",
      "Epoch 62/500 done!\n",
      "Training Loss: 1.5588829144835472\n",
      "Testing Loss: 1.6099933385849\n",
      "Epoch 63/500 done!\n",
      "Training Loss: 1.5976123064756393\n",
      "Testing Loss: 1.5927735567092896\n",
      "Epoch 64/500 done!\n",
      "Training Loss: 1.5972570404410362\n",
      "Testing Loss: 1.5769344568252563\n",
      "Epoch 65/500 done!\n",
      "Training Loss: 1.534482126434644\n",
      "Testing Loss: 1.560461401939392\n",
      "Epoch 66/500 done!\n",
      "Training Loss: 1.491313025355339\n",
      "Testing Loss: 1.5458921194076538\n",
      "Epoch 67/500 done!\n",
      "Training Loss: 1.531262993812561\n",
      "Testing Loss: 1.5311100482940674\n",
      "Epoch 68/500 done!\n",
      "Training Loss: 1.4726499343911807\n",
      "Testing Loss: 1.5164103507995605\n",
      "Epoch 69/500 done!\n",
      "Training Loss: 1.4576695660750072\n",
      "Testing Loss: 1.5023043155670166\n",
      "Epoch 70/500 done!\n",
      "Training Loss: 1.5166292985280354\n",
      "Testing Loss: 1.4887185096740723\n",
      "Epoch 71/500 done!\n",
      "Training Loss: 1.4540287479758263\n",
      "Testing Loss: 1.474063754081726\n",
      "Epoch 72/500 done!\n",
      "Training Loss: 1.4467111652096112\n",
      "Testing Loss: 1.4606225490570068\n",
      "Epoch 73/500 done!\n",
      "Training Loss: 1.4115494936704636\n",
      "Testing Loss: 1.4468997716903687\n",
      "Epoch 74/500 done!\n",
      "Training Loss: 1.415980964899063\n",
      "Testing Loss: 1.4340343475341797\n",
      "Epoch 75/500 done!\n",
      "Training Loss: 1.4047266244888306\n",
      "Testing Loss: 1.422325611114502\n",
      "Epoch 76/500 done!\n",
      "Training Loss: 1.3879859124620755\n",
      "Testing Loss: 1.4096876382827759\n",
      "Epoch 77/500 done!\n",
      "Training Loss: 1.3885308851798375\n",
      "Testing Loss: 1.3983701467514038\n",
      "Epoch 78/500 done!\n",
      "Training Loss: 1.3620546435316403\n",
      "Testing Loss: 1.3872933387756348\n",
      "Epoch 79/500 done!\n",
      "Training Loss: 1.3277731115619342\n",
      "Testing Loss: 1.3770191669464111\n",
      "Epoch 80/500 done!\n",
      "Training Loss: 1.3833646501104038\n",
      "Testing Loss: 1.3662889003753662\n",
      "Epoch 81/500 done!\n",
      "Training Loss: 1.2764921635389328\n",
      "Testing Loss: 1.3556526899337769\n",
      "Epoch 82/500 done!\n",
      "Training Loss: 1.303824596107006\n",
      "Testing Loss: 1.3456523418426514\n",
      "Epoch 83/500 done!\n",
      "Training Loss: 1.2680015886823337\n",
      "Testing Loss: 1.3361880779266357\n",
      "Epoch 84/500 done!\n",
      "Training Loss: 1.2637803976734479\n",
      "Testing Loss: 1.3274755477905273\n",
      "Epoch 85/500 done!\n",
      "Training Loss: 1.3047623882691066\n",
      "Testing Loss: 1.3189165592193604\n",
      "Epoch 86/500 done!\n",
      "Training Loss: 1.255518635114034\n",
      "Testing Loss: 1.3099086284637451\n",
      "Epoch 87/500 done!\n",
      "Training Loss: 1.247806042432785\n",
      "Testing Loss: 1.3013837337493896\n",
      "Epoch 88/500 done!\n",
      "Training Loss: 1.2115045512715976\n",
      "Testing Loss: 1.292824625968933\n",
      "Epoch 89/500 done!\n",
      "Training Loss: 1.2852314735452335\n",
      "Testing Loss: 1.2849382162094116\n",
      "Epoch 90/500 done!\n",
      "Training Loss: 1.2366605425874393\n",
      "Testing Loss: 1.276954174041748\n",
      "Epoch 91/500 done!\n",
      "Training Loss: 1.1813968122005463\n",
      "Testing Loss: 1.2698073387145996\n",
      "Epoch 92/500 done!\n",
      "Training Loss: 1.1995086893439293\n",
      "Testing Loss: 1.2627626657485962\n",
      "Epoch 93/500 done!\n",
      "Training Loss: 1.160859448214372\n",
      "Testing Loss: 1.256086826324463\n",
      "Epoch 94/500 done!\n",
      "Training Loss: 1.1761688987414043\n",
      "Testing Loss: 1.2489807605743408\n",
      "Epoch 95/500 done!\n",
      "Training Loss: 1.1488379711906116\n",
      "Testing Loss: 1.2424976825714111\n",
      "Epoch 96/500 done!\n",
      "Training Loss: 1.155196783443292\n",
      "Testing Loss: 1.2365777492523193\n",
      "Epoch 97/500 done!\n",
      "Training Loss: 1.1551211923360825\n",
      "Testing Loss: 1.2306969165802002\n",
      "Epoch 98/500 done!\n",
      "Training Loss: 1.1538543552160263\n",
      "Testing Loss: 1.2239089012145996\n",
      "Epoch 99/500 done!\n",
      "Training Loss: 1.1239777356386185\n",
      "Testing Loss: 1.2181037664413452\n",
      "Epoch 100/500 done!\n",
      "Training Loss: 1.1401308973630269\n",
      "Testing Loss: 1.2124443054199219\n",
      "Epoch 101/500 done!\n",
      "Training Loss: 1.1184898614883423\n",
      "Testing Loss: 1.2080020904541016\n",
      "Epoch 102/500 done!\n",
      "Training Loss: 1.1433627133568127\n",
      "Testing Loss: 1.2026336193084717\n",
      "Epoch 103/500 done!\n",
      "Training Loss: 1.1224639589587848\n",
      "Testing Loss: 1.1979228258132935\n",
      "Epoch 104/500 done!\n",
      "Training Loss: 1.0790657823284466\n",
      "Testing Loss: 1.1929569244384766\n",
      "Epoch 105/500 done!\n",
      "Training Loss: 1.1338155617316563\n",
      "Testing Loss: 1.1884928941726685\n",
      "Epoch 106/500 done!\n",
      "Training Loss: 1.0866125772396724\n",
      "Testing Loss: 1.1845288276672363\n",
      "Epoch 107/500 done!\n",
      "Training Loss: 1.0850845749179523\n",
      "Testing Loss: 1.1799330711364746\n",
      "Epoch 108/500 done!\n",
      "Training Loss: 1.0803280398249626\n",
      "Testing Loss: 1.1764031648635864\n",
      "Epoch 109/500 done!\n",
      "Training Loss: 1.0846436346570651\n",
      "Testing Loss: 1.1726257801055908\n",
      "Epoch 110/500 done!\n",
      "Training Loss: 1.0852981234590213\n",
      "Testing Loss: 1.1686434745788574\n",
      "Epoch 111/500 done!\n",
      "Training Loss: 1.1055771311124165\n",
      "Testing Loss: 1.1647831201553345\n",
      "Epoch 112/500 done!\n",
      "Training Loss: 1.0676482294996579\n",
      "Testing Loss: 1.1617405414581299\n",
      "Epoch 113/500 done!\n",
      "Training Loss: 1.0434215888381004\n",
      "Testing Loss: 1.1580852270126343\n",
      "Epoch 114/500 done!\n",
      "Training Loss: 1.0999075869719188\n",
      "Testing Loss: 1.1552197933197021\n",
      "Epoch 115/500 done!\n",
      "Training Loss: 1.0635203172763188\n",
      "Testing Loss: 1.1521825790405273\n",
      "Epoch 116/500 done!\n",
      "Training Loss: 1.082303874194622\n",
      "Testing Loss: 1.1487537622451782\n",
      "Epoch 117/500 done!\n",
      "Training Loss: 1.0880543092886608\n",
      "Testing Loss: 1.1466867923736572\n",
      "Epoch 118/500 done!\n",
      "Training Loss: 1.0453815534710884\n",
      "Testing Loss: 1.144774317741394\n",
      "Epoch 119/500 done!\n",
      "Training Loss: 1.0386098474264145\n",
      "Testing Loss: 1.142252802848816\n",
      "Epoch 120/500 done!\n",
      "Training Loss: 1.0447584390640259\n",
      "Testing Loss: 1.139228105545044\n",
      "Epoch 121/500 done!\n",
      "Training Loss: 1.0437036454677582\n",
      "Testing Loss: 1.136887788772583\n",
      "Epoch 122/500 done!\n",
      "Training Loss: 1.0242834364374478\n",
      "Testing Loss: 1.1346467733383179\n",
      "Epoch 123/500 done!\n",
      "Training Loss: 1.018992508451144\n",
      "Testing Loss: 1.1325150728225708\n",
      "Epoch 124/500 done!\n",
      "Training Loss: 1.0513180842002232\n",
      "Testing Loss: 1.1297603845596313\n",
      "Epoch 125/500 done!\n",
      "Training Loss: 1.0365697865684826\n",
      "Testing Loss: 1.1275566816329956\n",
      "Epoch 126/500 done!\n",
      "Training Loss: 1.020703633626302\n",
      "Testing Loss: 1.1256325244903564\n",
      "Epoch 127/500 done!\n",
      "Training Loss: 1.0077432865897815\n",
      "Testing Loss: 1.1237027645111084\n",
      "Epoch 128/500 done!\n",
      "Training Loss: 1.0141056502858798\n",
      "Testing Loss: 1.1214354038238525\n",
      "Epoch 129/500 done!\n",
      "Training Loss: 1.024049811065197\n",
      "Testing Loss: 1.1198097467422485\n",
      "Epoch 130/500 done!\n",
      "Training Loss: 1.012913815677166\n",
      "Testing Loss: 1.118727684020996\n",
      "Epoch 131/500 done!\n",
      "Training Loss: 1.0413713430364926\n",
      "Testing Loss: 1.117578148841858\n",
      "Epoch 132/500 done!\n",
      "Training Loss: 1.0044026349981625\n",
      "Testing Loss: 1.116119146347046\n",
      "Epoch 133/500 done!\n",
      "Training Loss: 1.0294062718749046\n",
      "Testing Loss: 1.1150200366973877\n",
      "Epoch 134/500 done!\n",
      "Training Loss: 0.9989406416813532\n",
      "Testing Loss: 1.1131788492202759\n",
      "Epoch 135/500 done!\n",
      "Training Loss: 0.9974621087312698\n",
      "Testing Loss: 1.1116777658462524\n",
      "Epoch 136/500 done!\n",
      "Training Loss: 1.0184653550386429\n",
      "Testing Loss: 1.1102863550186157\n",
      "Epoch 137/500 done!\n",
      "Training Loss: 1.0336171016097069\n",
      "Testing Loss: 1.109605312347412\n",
      "Epoch 138/500 done!\n",
      "Training Loss: 1.0072873383760452\n",
      "Testing Loss: 1.1080189943313599\n",
      "Epoch 139/500 done!\n",
      "Training Loss: 0.9779602463046709\n",
      "Testing Loss: 1.1080825328826904\n",
      "Epoch 140/500 done!\n",
      "Training Loss: 0.9866002351045609\n",
      "Testing Loss: 1.1069101095199585\n",
      "Epoch 141/500 done!\n",
      "Training Loss: 0.9786378939946493\n",
      "Testing Loss: 1.1057054996490479\n",
      "Epoch 142/500 done!\n",
      "Training Loss: 0.983978326121966\n",
      "Testing Loss: 1.105167269706726\n",
      "Epoch 143/500 done!\n",
      "Training Loss: 1.0071330641706784\n",
      "Testing Loss: 1.104595422744751\n",
      "Epoch 144/500 done!\n",
      "Training Loss: 0.9903946990768114\n",
      "Testing Loss: 1.1040796041488647\n",
      "Epoch 145/500 done!\n",
      "Training Loss: 1.0153841252128284\n",
      "Testing Loss: 1.1033110618591309\n",
      "Epoch 146/500 done!\n",
      "Training Loss: 0.9927347972989082\n",
      "Testing Loss: 1.102665901184082\n",
      "Epoch 147/500 done!\n",
      "Training Loss: 0.9877092639605204\n",
      "Testing Loss: 1.1017898321151733\n",
      "Epoch 148/500 done!\n",
      "Training Loss: 0.9750181833902994\n",
      "Testing Loss: 1.1017557382583618\n",
      "Epoch 149/500 done!\n",
      "Training Loss: 0.975596122443676\n",
      "Testing Loss: 1.1010496616363525\n",
      "Epoch 150/500 done!\n",
      "Training Loss: 0.9841224278012911\n",
      "Testing Loss: 1.1005091667175293\n",
      "Epoch 151/500 done!\n",
      "Training Loss: 0.9772383744517962\n",
      "Testing Loss: 1.1000549793243408\n",
      "Epoch 152/500 done!\n",
      "Training Loss: 0.980091060201327\n",
      "Testing Loss: 1.0990897417068481\n",
      "Epoch 153/500 done!\n",
      "Training Loss: 1.0029811337590218\n",
      "Testing Loss: 1.0986124277114868\n",
      "Epoch 154/500 done!\n",
      "Training Loss: 0.9711186811327934\n",
      "Testing Loss: 1.0987108945846558\n",
      "Epoch 155/500 done!\n",
      "Training Loss: 0.981592170894146\n",
      "Testing Loss: 1.0987367630004883\n",
      "Epoch 156/500 done!\n",
      "Training Loss: 0.9816809619466463\n",
      "Testing Loss: 1.0987601280212402\n",
      "Epoch 157/500 done!\n",
      "Training Loss: 1.017569586634636\n",
      "Testing Loss: 1.0984113216400146\n",
      "Epoch 158/500 done!\n",
      "Training Loss: 1.0068065772453945\n",
      "Testing Loss: 1.0974606275558472\n",
      "Epoch 159/500 done!\n",
      "Training Loss: 1.0265425940354664\n",
      "Testing Loss: 1.097325086593628\n",
      "Epoch 160/500 done!\n",
      "Training Loss: 0.9755089009801546\n",
      "Testing Loss: 1.0973477363586426\n",
      "Epoch 161/500 done!\n",
      "Training Loss: 0.9685216322541237\n",
      "Testing Loss: 1.0966273546218872\n",
      "Epoch 162/500 done!\n",
      "Training Loss: 0.9843181992570559\n",
      "Testing Loss: 1.0968328714370728\n",
      "Epoch 163/500 done!\n",
      "Training Loss: 0.9871752882997195\n",
      "Testing Loss: 1.0965399742126465\n",
      "Epoch 164/500 done!\n",
      "Training Loss: 0.9576813702781996\n",
      "Testing Loss: 1.0964964628219604\n",
      "Epoch 165/500 done!\n",
      "Training Loss: 0.9728621939818064\n",
      "Testing Loss: 1.0963410139083862\n",
      "Epoch 166/500 done!\n",
      "Training Loss: 0.967144658168157\n",
      "Testing Loss: 1.096068024635315\n",
      "Epoch 167/500 done!\n",
      "Training Loss: 0.9550244559844335\n",
      "Testing Loss: 1.0956701040267944\n",
      "Epoch 168/500 done!\n",
      "Training Loss: 0.9576569969455401\n",
      "Testing Loss: 1.095259666442871\n",
      "Epoch 169/500 done!\n",
      "Training Loss: 0.9556924005349478\n",
      "Testing Loss: 1.0948466062545776\n",
      "Epoch 170/500 done!\n",
      "Training Loss: 0.9811012844244639\n",
      "Testing Loss: 1.0943514108657837\n",
      "Epoch 171/500 done!\n",
      "Training Loss: 0.968426026403904\n",
      "Testing Loss: 1.0941340923309326\n",
      "Epoch 172/500 done!\n",
      "Training Loss: 0.9792313228050867\n",
      "Testing Loss: 1.09451425075531\n",
      "Epoch 173/500 done!\n",
      "Training Loss: 0.9863520910342535\n",
      "Testing Loss: 1.0939948558807373\n",
      "Epoch 174/500 done!\n",
      "Training Loss: 0.952577106654644\n",
      "Testing Loss: 1.0942935943603516\n",
      "Epoch 175/500 done!\n",
      "Training Loss: 0.9567785163720449\n",
      "Testing Loss: 1.0937535762786865\n",
      "Epoch 176/500 done!\n",
      "Training Loss: 0.9818102916081747\n",
      "Testing Loss: 1.0940877199172974\n",
      "Epoch 177/500 done!\n",
      "Training Loss: 0.9720133195320765\n",
      "Testing Loss: 1.0929673910140991\n",
      "Epoch 178/500 done!\n",
      "Training Loss: 0.9587602814038595\n",
      "Testing Loss: 1.0931713581085205\n",
      "Epoch 179/500 done!\n",
      "Training Loss: 0.9714437400301298\n",
      "Testing Loss: 1.0934209823608398\n",
      "Epoch 180/500 done!\n",
      "Training Loss: 0.9843419020374616\n",
      "Testing Loss: 1.0925637483596802\n",
      "Epoch 181/500 done!\n",
      "Training Loss: 0.9791687900821368\n",
      "Testing Loss: 1.0923540592193604\n",
      "Epoch 182/500 done!\n",
      "Training Loss: 0.9714644948641459\n",
      "Testing Loss: 1.0918352603912354\n",
      "Epoch 183/500 done!\n",
      "Training Loss: 0.9649627630909284\n",
      "Testing Loss: 1.0915210247039795\n",
      "Epoch 184/500 done!\n",
      "Training Loss: 0.9861986363927523\n",
      "Testing Loss: 1.0915461778640747\n",
      "Epoch 185/500 done!\n",
      "Training Loss: 0.9674852093060812\n",
      "Testing Loss: 1.0914461612701416\n",
      "Epoch 186/500 done!\n",
      "Training Loss: 0.9785463586449623\n",
      "Testing Loss: 1.0916804075241089\n",
      "Epoch 187/500 done!\n",
      "Training Loss: 0.9556367173790932\n",
      "Testing Loss: 1.092069387435913\n",
      "Epoch 188/500 done!\n",
      "Training Loss: 0.9644989122947057\n",
      "Testing Loss: 1.0919233560562134\n",
      "Epoch 189/500 done!\n",
      "Training Loss: 0.9590141686300436\n",
      "Testing Loss: 1.0915707349777222\n",
      "Epoch 190/500 done!\n",
      "Training Loss: 0.9801691696047783\n",
      "Testing Loss: 1.0916173458099365\n",
      "Epoch 191/500 done!\n",
      "Training Loss: 0.9542781238754591\n",
      "Testing Loss: 1.0904535055160522\n",
      "Epoch 192/500 done!\n",
      "Training Loss: 0.9734257683157921\n",
      "Testing Loss: 1.0898492336273193\n",
      "Epoch 193/500 done!\n",
      "Training Loss: 0.9648338953653971\n",
      "Testing Loss: 1.0900365114212036\n",
      "Epoch 194/500 done!\n",
      "Training Loss: 0.9597777376572291\n",
      "Testing Loss: 1.0894533395767212\n",
      "Epoch 195/500 done!\n",
      "Training Loss: 0.9871773893634478\n",
      "Testing Loss: 1.0898020267486572\n",
      "Epoch 196/500 done!\n",
      "Training Loss: 0.9561749299367269\n",
      "Testing Loss: 1.09065842628479\n",
      "Epoch 197/500 done!\n",
      "Training Loss: 0.965457613269488\n",
      "Testing Loss: 1.0907877683639526\n",
      "Epoch 198/500 done!\n",
      "Training Loss: 0.9525944367051125\n",
      "Testing Loss: 1.0908974409103394\n",
      "Epoch 199/500 done!\n",
      "Training Loss: 0.9655956899126371\n",
      "Testing Loss: 1.0904301404953003\n",
      "Epoch 200/500 done!\n",
      "Training Loss: 0.9554227118690809\n",
      "Testing Loss: 1.0905007123947144\n",
      "Epoch 201/500 done!\n",
      "Training Loss: 1.0008205026388168\n",
      "Testing Loss: 1.0908247232437134\n",
      "Epoch 202/500 done!\n",
      "Training Loss: 1.009580967326959\n",
      "Testing Loss: 1.0901850461959839\n",
      "Epoch 203/500 done!\n",
      "Training Loss: 0.9551237399379412\n",
      "Testing Loss: 1.0909781455993652\n",
      "Epoch 204/500 done!\n",
      "Training Loss: 0.9585333292682966\n",
      "Testing Loss: 1.0904806852340698\n",
      "Epoch 205/500 done!\n",
      "Training Loss: 0.9641253997882208\n",
      "Testing Loss: 1.0908615589141846\n",
      "Epoch 206/500 done!\n",
      "Training Loss: 0.9435354719559351\n",
      "Testing Loss: 1.0911130905151367\n",
      "Epoch 207/500 done!\n",
      "Training Loss: 0.9500327284137408\n",
      "Testing Loss: 1.091335654258728\n",
      "Epoch 208/500 done!\n",
      "Training Loss: 1.0082900325457256\n",
      "Testing Loss: 1.09164297580719\n",
      "Epoch 209/500 done!\n",
      "Training Loss: 0.9448095063368479\n",
      "Testing Loss: 1.0907179117202759\n",
      "Epoch 210/500 done!\n",
      "Training Loss: 0.9521375770370165\n",
      "Testing Loss: 1.0905483961105347\n",
      "Epoch 211/500 done!\n",
      "Training Loss: 0.9578930859764417\n",
      "Testing Loss: 1.0900344848632812\n",
      "Epoch 212/500 done!\n",
      "Training Loss: 0.9463440353671709\n",
      "Testing Loss: 1.090093731880188\n",
      "Epoch 213/500 done!\n",
      "Training Loss: 0.9677913611133894\n",
      "Testing Loss: 1.0902736186981201\n",
      "Epoch 214/500 done!\n",
      "Training Loss: 0.948765034476916\n",
      "Testing Loss: 1.090729832649231\n",
      "Epoch 215/500 done!\n",
      "Training Loss: 0.9427189777294794\n",
      "Testing Loss: 1.0908260345458984\n",
      "Epoch 216/500 done!\n",
      "Training Loss: 0.9542808930079142\n",
      "Testing Loss: 1.090867519378662\n",
      "Epoch 217/500 done!\n",
      "Training Loss: 0.96856456498305\n",
      "Testing Loss: 1.090634822845459\n",
      "Epoch 218/500 done!\n",
      "Training Loss: 0.9402002592881521\n",
      "Testing Loss: 1.0896447896957397\n",
      "Epoch 219/500 done!\n",
      "Training Loss: 0.9499221046765646\n",
      "Testing Loss: 1.089403748512268\n",
      "Epoch 220/500 done!\n",
      "Training Loss: 0.9548522358139356\n",
      "Testing Loss: 1.0894454717636108\n",
      "Epoch 221/500 done!\n",
      "Training Loss: 0.9521851812799772\n",
      "Testing Loss: 1.089492917060852\n",
      "Epoch 222/500 done!\n",
      "Training Loss: 0.9429688826203346\n",
      "Testing Loss: 1.089889645576477\n",
      "Epoch 223/500 done!\n",
      "Training Loss: 0.9607705076535543\n",
      "Testing Loss: 1.089605689048767\n",
      "Epoch 224/500 done!\n",
      "Training Loss: 0.9502886136372884\n",
      "Testing Loss: 1.0884044170379639\n",
      "Epoch 225/500 done!\n",
      "Training Loss: 0.947871079047521\n",
      "Testing Loss: 1.0884063243865967\n",
      "Epoch 226/500 done!\n",
      "Training Loss: 0.9765377839406332\n",
      "Testing Loss: 1.0885465145111084\n",
      "Epoch 227/500 done!\n",
      "Training Loss: 0.960803618033727\n",
      "Testing Loss: 1.0882247686386108\n",
      "Epoch 228/500 done!\n",
      "Training Loss: 0.9493760044376055\n",
      "Testing Loss: 1.0892789363861084\n",
      "Epoch 229/500 done!\n",
      "Training Loss: 0.9438565572102865\n",
      "Testing Loss: 1.0899232625961304\n",
      "Epoch 230/500 done!\n",
      "Training Loss: 0.9647274017333984\n",
      "Testing Loss: 1.0894941091537476\n",
      "Epoch 231/500 done!\n",
      "Training Loss: 0.9548181891441345\n",
      "Testing Loss: 1.090206503868103\n",
      "Epoch 232/500 done!\n",
      "Training Loss: 0.9347450062632561\n",
      "Testing Loss: 1.090040922164917\n",
      "Epoch 233/500 done!\n",
      "Training Loss: 0.9735399037599564\n",
      "Testing Loss: 1.090273141860962\n",
      "Epoch 234/500 done!\n",
      "Training Loss: 0.9570972224076589\n",
      "Testing Loss: 1.0908229351043701\n",
      "Epoch 235/500 done!\n",
      "Training Loss: 0.9512293338775635\n",
      "Testing Loss: 1.0898027420043945\n",
      "Epoch 236/500 done!\n",
      "Training Loss: 0.9393191561102867\n",
      "Testing Loss: 1.0895582437515259\n",
      "Epoch 237/500 done!\n",
      "Training Loss: 0.9588948413729668\n",
      "Testing Loss: 1.0897258520126343\n",
      "Epoch 238/500 done!\n",
      "Training Loss: 0.9595517540971438\n",
      "Testing Loss: 1.0896522998809814\n",
      "Epoch 239/500 done!\n",
      "Training Loss: 0.9706485097606977\n",
      "Testing Loss: 1.0895462036132812\n",
      "Epoch 240/500 done!\n",
      "Training Loss: 0.9622569680213928\n",
      "Testing Loss: 1.0889166593551636\n",
      "Epoch 241/500 done!\n",
      "Training Loss: 0.9772643744945526\n",
      "Testing Loss: 1.088735580444336\n",
      "Epoch 242/500 done!\n",
      "Training Loss: 0.947354311744372\n",
      "Testing Loss: 1.089374303817749\n",
      "Epoch 243/500 done!\n",
      "Training Loss: 0.9387963612874349\n",
      "Testing Loss: 1.0890083312988281\n",
      "Epoch 244/500 done!\n",
      "Training Loss: 0.9490971465905508\n",
      "Testing Loss: 1.0890018939971924\n",
      "Epoch 245/500 done!\n",
      "Training Loss: 0.9468624045451483\n",
      "Testing Loss: 1.0897583961486816\n",
      "Epoch 246/500 done!\n",
      "Training Loss: 0.9422947640220324\n",
      "Testing Loss: 1.0903544425964355\n",
      "Epoch 247/500 done!\n",
      "Training Loss: 0.9582646215955416\n",
      "Testing Loss: 1.091020941734314\n",
      "Epoch 248/500 done!\n",
      "Training Loss: 0.9515308936436971\n",
      "Testing Loss: 1.0902060270309448\n",
      "Epoch 249/500 done!\n",
      "Training Loss: 0.9484679450591406\n",
      "Testing Loss: 1.090489387512207\n",
      "Epoch 250/500 done!\n",
      "Training Loss: 0.9338284383217493\n",
      "Testing Loss: 1.0899932384490967\n",
      "Epoch 251/500 done!\n",
      "Training Loss: 0.9403093084692955\n",
      "Testing Loss: 1.0896834135055542\n",
      "Epoch 252/500 done!\n",
      "Training Loss: 0.930876779059569\n",
      "Testing Loss: 1.0897382497787476\n",
      "Epoch 253/500 done!\n",
      "Training Loss: 0.934441956381003\n",
      "Testing Loss: 1.0895488262176514\n",
      "Epoch 254/500 done!\n",
      "Training Loss: 0.9521560470263163\n",
      "Testing Loss: 1.089046835899353\n",
      "Epoch 255/500 done!\n",
      "Training Loss: 0.9717630669474602\n",
      "Testing Loss: 1.08970046043396\n",
      "Epoch 256/500 done!\n",
      "Training Loss: 0.9299650862812996\n",
      "Testing Loss: 1.0900921821594238\n",
      "Epoch 257/500 done!\n",
      "Training Loss: 0.9372264718015989\n",
      "Testing Loss: 1.0899463891983032\n",
      "Epoch 258/500 done!\n",
      "Training Loss: 0.9379635080695152\n",
      "Testing Loss: 1.0896230936050415\n",
      "Epoch 259/500 done!\n",
      "Training Loss: 0.9296303242444992\n",
      "Testing Loss: 1.0890142917633057\n",
      "Epoch 260/500 done!\n",
      "Training Loss: 0.9538860743244489\n",
      "Testing Loss: 1.0886870622634888\n",
      "Epoch 261/500 done!\n",
      "Training Loss: 0.9253176525235176\n",
      "Testing Loss: 1.0884288549423218\n",
      "Epoch 262/500 done!\n",
      "Training Loss: 0.9412357583642006\n",
      "Testing Loss: 1.0881636142730713\n",
      "Epoch 263/500 done!\n",
      "Training Loss: 0.9403352936108907\n",
      "Testing Loss: 1.0882062911987305\n",
      "Epoch 264/500 done!\n",
      "Training Loss: 0.9186742305755615\n",
      "Testing Loss: 1.0880422592163086\n",
      "Epoch 265/500 done!\n",
      "Training Loss: 0.9335092380642891\n",
      "Testing Loss: 1.0884041786193848\n",
      "Epoch 266/500 done!\n",
      "Training Loss: 0.9320631970961889\n",
      "Testing Loss: 1.0872465372085571\n",
      "Epoch 267/500 done!\n",
      "Training Loss: 0.9300562689701716\n",
      "Testing Loss: 1.0870490074157715\n",
      "Epoch 268/500 done!\n",
      "Training Loss: 0.9707758550842603\n",
      "Testing Loss: 1.0868686437606812\n",
      "Epoch 269/500 done!\n",
      "Training Loss: 0.9375715429584185\n",
      "Testing Loss: 1.086835503578186\n",
      "Epoch 270/500 done!\n",
      "Training Loss: 0.9693281898895899\n",
      "Testing Loss: 1.0871243476867676\n",
      "Epoch 271/500 done!\n",
      "Training Loss: 0.9391423612833023\n",
      "Testing Loss: 1.0881119966506958\n",
      "Epoch 272/500 done!\n",
      "Training Loss: 0.9468143060803413\n",
      "Testing Loss: 1.0869488716125488\n",
      "Epoch 273/500 done!\n",
      "Training Loss: 0.9384070212642351\n",
      "Testing Loss: 1.087070345878601\n",
      "Epoch 274/500 done!\n",
      "Training Loss: 0.9211359893282255\n",
      "Testing Loss: 1.0862071514129639\n",
      "Epoch 275/500 done!\n",
      "Training Loss: 0.9354353174567223\n",
      "Testing Loss: 1.0873507261276245\n",
      "Epoch 276/500 done!\n",
      "Training Loss: 0.9572173580527306\n",
      "Testing Loss: 1.0877816677093506\n",
      "Epoch 277/500 done!\n",
      "Training Loss: 0.9239164665341377\n",
      "Testing Loss: 1.0878537893295288\n",
      "Epoch 278/500 done!\n",
      "Training Loss: 0.937352808813254\n",
      "Testing Loss: 1.0879563093185425\n",
      "Epoch 279/500 done!\n",
      "Training Loss: 0.9380852356553078\n",
      "Testing Loss: 1.0876665115356445\n",
      "Epoch 280/500 done!\n",
      "Training Loss: 0.9597172265251478\n",
      "Testing Loss: 1.0874457359313965\n",
      "Epoch 281/500 done!\n",
      "Training Loss: 0.937034048140049\n",
      "Testing Loss: 1.0876637697219849\n",
      "Epoch 282/500 done!\n",
      "Training Loss: 0.9315044631560644\n",
      "Testing Loss: 1.0882103443145752\n",
      "Epoch 283/500 done!\n",
      "Training Loss: 0.9265922283132871\n",
      "Testing Loss: 1.0880045890808105\n",
      "Epoch 284/500 done!\n",
      "Training Loss: 0.9334750672181448\n",
      "Testing Loss: 1.0880694389343262\n",
      "Epoch 285/500 done!\n",
      "Training Loss: 0.9325329586863518\n",
      "Testing Loss: 1.087852120399475\n",
      "Epoch 286/500 done!\n",
      "Training Loss: 0.9328053072094917\n",
      "Testing Loss: 1.0866470336914062\n",
      "Epoch 287/500 done!\n",
      "Training Loss: 0.9189313724637032\n",
      "Testing Loss: 1.0867791175842285\n",
      "Epoch 288/500 done!\n",
      "Training Loss: 0.9389771769444147\n",
      "Testing Loss: 1.0872756242752075\n",
      "Epoch 289/500 done!\n",
      "Training Loss: 0.9292629137635231\n",
      "Testing Loss: 1.08810293674469\n",
      "Epoch 290/500 done!\n",
      "Training Loss: 0.9362302099665006\n",
      "Testing Loss: 1.0878335237503052\n",
      "Epoch 291/500 done!\n",
      "Training Loss: 0.9206433668732643\n",
      "Testing Loss: 1.0885981321334839\n",
      "Epoch 292/500 done!\n",
      "Training Loss: 0.9266493568817774\n",
      "Testing Loss: 1.0885626077651978\n",
      "Epoch 293/500 done!\n",
      "Training Loss: 0.9417306333780289\n",
      "Testing Loss: 1.0889828205108643\n",
      "Epoch 294/500 done!\n",
      "Training Loss: 0.9160739493866762\n",
      "Testing Loss: 1.088910460472107\n",
      "Epoch 295/500 done!\n",
      "Training Loss: 0.9459187611937523\n",
      "Testing Loss: 1.0887929201126099\n",
      "Epoch 296/500 done!\n",
      "Training Loss: 0.9478974392016729\n",
      "Testing Loss: 1.09026038646698\n",
      "Epoch 297/500 done!\n",
      "Training Loss: 0.9365612864494324\n",
      "Testing Loss: 1.090055227279663\n",
      "Epoch 298/500 done!\n",
      "Training Loss: 0.9157203088204066\n",
      "Testing Loss: 1.0892795324325562\n",
      "Epoch 299/500 done!\n",
      "Training Loss: 0.9327675923705101\n",
      "Testing Loss: 1.0891340970993042\n",
      "Epoch 300/500 done!\n",
      "Training Loss: 0.9229592680931091\n",
      "Testing Loss: 1.0886954069137573\n",
      "Epoch 301/500 done!\n",
      "Training Loss: 0.9304062724113464\n",
      "Testing Loss: 1.0895037651062012\n",
      "Epoch 302/500 done!\n",
      "Training Loss: 0.9377348646521568\n",
      "Testing Loss: 1.088653326034546\n",
      "Epoch 303/500 done!\n",
      "Training Loss: 0.9288722897569338\n",
      "Testing Loss: 1.0877395868301392\n",
      "Epoch 304/500 done!\n",
      "Training Loss: 0.920956256488959\n",
      "Testing Loss: 1.0878543853759766\n",
      "Epoch 305/500 done!\n",
      "Training Loss: 0.9321552018324534\n",
      "Testing Loss: 1.0875121355056763\n",
      "Epoch 306/500 done!\n",
      "Training Loss: 0.932643490533034\n",
      "Testing Loss: 1.0882223844528198\n",
      "Epoch 307/500 done!\n",
      "Training Loss: 0.926233192284902\n",
      "Testing Loss: 1.0885698795318604\n",
      "Epoch 308/500 done!\n",
      "Training Loss: 0.9201780458291372\n",
      "Testing Loss: 1.0883066654205322\n",
      "Epoch 309/500 done!\n",
      "Training Loss: 0.929165410498778\n",
      "Testing Loss: 1.0872880220413208\n",
      "Epoch 310/500 done!\n",
      "Training Loss: 0.9286099324623743\n",
      "Testing Loss: 1.0879273414611816\n",
      "Epoch 311/500 done!\n",
      "Training Loss: 0.9201659734050432\n",
      "Testing Loss: 1.087887167930603\n",
      "Epoch 312/500 done!\n",
      "Training Loss: 0.9135052139560381\n",
      "Testing Loss: 1.0892139673233032\n",
      "Epoch 313/500 done!\n",
      "Training Loss: 0.9587123269836108\n",
      "Testing Loss: 1.0892592668533325\n",
      "Epoch 314/500 done!\n",
      "Training Loss: 0.9187538425127665\n",
      "Testing Loss: 1.0890694856643677\n",
      "Epoch 315/500 done!\n",
      "Training Loss: 0.9150878811875979\n",
      "Testing Loss: 1.0885703563690186\n",
      "Epoch 316/500 done!\n",
      "Training Loss: 0.9415139257907867\n",
      "Testing Loss: 1.0887700319290161\n",
      "Epoch 317/500 done!\n",
      "Training Loss: 0.9206962957978249\n",
      "Testing Loss: 1.088297724723816\n",
      "Epoch 318/500 done!\n",
      "Training Loss: 0.9074888775746027\n",
      "Testing Loss: 1.0880804061889648\n",
      "Epoch 319/500 done!\n",
      "Training Loss: 0.933218906323115\n",
      "Testing Loss: 1.0877666473388672\n",
      "Epoch 320/500 done!\n",
      "Training Loss: 0.9331596121191978\n",
      "Testing Loss: 1.0873781442642212\n",
      "Epoch 321/500 done!\n",
      "Training Loss: 0.9265542402863503\n",
      "Testing Loss: 1.0874130725860596\n",
      "Epoch 322/500 done!\n",
      "Training Loss: 0.9214653397599856\n",
      "Testing Loss: 1.08761465549469\n",
      "Epoch 323/500 done!\n",
      "Training Loss: 0.9095307861765226\n",
      "Testing Loss: 1.0877039432525635\n",
      "Epoch 324/500 done!\n",
      "Training Loss: 0.9185147558649381\n",
      "Testing Loss: 1.0874384641647339\n",
      "Epoch 325/500 done!\n",
      "Training Loss: 0.9202576180299123\n",
      "Testing Loss: 1.0878880023956299\n",
      "Epoch 326/500 done!\n",
      "Training Loss: 0.9381653194626173\n",
      "Testing Loss: 1.0880212783813477\n",
      "Epoch 327/500 done!\n",
      "Training Loss: 0.9310363481442133\n",
      "Testing Loss: 1.0892382860183716\n",
      "Epoch 328/500 done!\n",
      "Training Loss: 0.9509129822254181\n",
      "Testing Loss: 1.089450716972351\n",
      "Epoch 329/500 done!\n",
      "Training Loss: 0.9208857963482538\n",
      "Testing Loss: 1.0886319875717163\n",
      "Epoch 330/500 done!\n",
      "Training Loss: 0.9288366734981537\n",
      "Testing Loss: 1.0894588232040405\n",
      "Epoch 331/500 done!\n",
      "Training Loss: 0.9385661780834198\n",
      "Testing Loss: 1.088627576828003\n",
      "Epoch 332/500 done!\n",
      "Training Loss: 0.9050548821687698\n",
      "Testing Loss: 1.0889774560928345\n",
      "Epoch 333/500 done!\n",
      "Training Loss: 0.905940609673659\n",
      "Testing Loss: 1.0884521007537842\n",
      "Epoch 334/500 done!\n",
      "Training Loss: 0.9076453894376755\n",
      "Testing Loss: 1.0892208814620972\n",
      "Epoch 335/500 done!\n",
      "Training Loss: 0.9132946009437243\n",
      "Testing Loss: 1.0890507698059082\n",
      "Epoch 336/500 done!\n",
      "Training Loss: 0.9504160632689794\n",
      "Testing Loss: 1.0894068479537964\n",
      "Epoch 337/500 done!\n",
      "Training Loss: 0.9064671446879705\n",
      "Testing Loss: 1.0898866653442383\n",
      "Epoch 338/500 done!\n",
      "Training Loss: 0.9233430077632269\n",
      "Testing Loss: 1.0904291868209839\n",
      "Epoch 339/500 done!\n",
      "Training Loss: 0.892801729341348\n",
      "Testing Loss: 1.090175986289978\n",
      "Epoch 340/500 done!\n",
      "Training Loss: 0.920804999768734\n",
      "Testing Loss: 1.0910019874572754\n",
      "Epoch 341/500 done!\n",
      "Training Loss: 0.9522585620482763\n",
      "Testing Loss: 1.0917271375656128\n",
      "Epoch 342/500 done!\n",
      "Training Loss: 0.9282373239596685\n",
      "Testing Loss: 1.0920689105987549\n",
      "Epoch 343/500 done!\n",
      "Training Loss: 0.9044487128655115\n",
      "Testing Loss: 1.092085599899292\n",
      "Epoch 344/500 done!\n",
      "Training Loss: 0.9076572706302007\n",
      "Testing Loss: 1.0920472145080566\n",
      "Epoch 345/500 done!\n",
      "Training Loss: 0.9220413068930308\n",
      "Testing Loss: 1.0919398069381714\n",
      "Epoch 346/500 done!\n",
      "Training Loss: 0.9164730732639631\n",
      "Testing Loss: 1.0920774936676025\n",
      "Epoch 347/500 done!\n",
      "Training Loss: 0.9115137284000715\n",
      "Testing Loss: 1.0916941165924072\n",
      "Epoch 348/500 done!\n",
      "Training Loss: 0.9134437739849091\n",
      "Testing Loss: 1.091545820236206\n",
      "Epoch 349/500 done!\n",
      "Training Loss: 0.9168142303824425\n",
      "Testing Loss: 1.0906727313995361\n",
      "Epoch 350/500 done!\n",
      "Training Loss: 0.9007645845413208\n",
      "Testing Loss: 1.0910873413085938\n",
      "Epoch 351/500 done!\n",
      "Training Loss: 0.9332938119769096\n",
      "Testing Loss: 1.0903534889221191\n",
      "Epoch 352/500 done!\n",
      "Training Loss: 0.8796006167928377\n",
      "Testing Loss: 1.0906562805175781\n",
      "Epoch 353/500 done!\n",
      "Training Loss: 0.9326460510492325\n",
      "Testing Loss: 1.0909467935562134\n",
      "Epoch 354/500 done!\n",
      "Training Loss: 0.910182997584343\n",
      "Testing Loss: 1.0922420024871826\n",
      "Epoch 355/500 done!\n",
      "Training Loss: 0.9246601089835167\n",
      "Testing Loss: 1.091831922531128\n",
      "Epoch 356/500 done!\n",
      "Training Loss: 0.9044859036803246\n",
      "Testing Loss: 1.0928665399551392\n",
      "Epoch 357/500 done!\n",
      "Training Loss: 0.8957687243819237\n",
      "Testing Loss: 1.0925589799880981\n",
      "Epoch 358/500 done!\n",
      "Training Loss: 0.8937098359068235\n",
      "Testing Loss: 1.0934847593307495\n",
      "Epoch 359/500 done!\n",
      "Training Loss: 0.8907677158713341\n",
      "Testing Loss: 1.094400405883789\n",
      "Epoch 360/500 done!\n",
      "Training Loss: 0.8946051721771558\n",
      "Testing Loss: 1.0952200889587402\n",
      "Epoch 361/500 done!\n",
      "Training Loss: 0.8923030445973078\n",
      "Testing Loss: 1.095723032951355\n",
      "Epoch 362/500 done!\n",
      "Training Loss: 0.8893614535530409\n",
      "Testing Loss: 1.096094012260437\n",
      "Epoch 363/500 done!\n",
      "Training Loss: 0.9089328522483507\n",
      "Testing Loss: 1.0958739519119263\n",
      "Epoch 364/500 done!\n",
      "Training Loss: 0.9110476945837339\n",
      "Testing Loss: 1.095595359802246\n",
      "Epoch 365/500 done!\n",
      "Training Loss: 0.9038021589318911\n",
      "Testing Loss: 1.0956850051879883\n",
      "Epoch 366/500 done!\n",
      "Training Loss: 0.8895670051376025\n",
      "Testing Loss: 1.0972710847854614\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_epi, predicted_index = model(test_interaction_vars, test_pm25_cont, test_controlled_vars)\n",
    "    loss_test = mseLoss(predicted_epi, test_pheno_epi)\n",
    "    print(\"Testing Loss: {}\".format(loss_test.item()))\n",
    "early_stop_trigger_counter = 0\n",
    "early_stop_tolerance = 0.2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, sample in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        predicted_epi, predicted_index = model(sample[\"interaction_input_vars\"], sample[\"interactor_var\"], sample[\"controlled_vars\"])\n",
    "        label = torch.unsqueeze(sample[\"label\"], 1)\n",
    "        loss = mseLoss(predicted_epi, label)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # print average loss for epoch\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # evaluation on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_epi, predicted_interaction = model(test_interaction_vars, test_pm25_cont, test_controlled_vars)\n",
    "        loss_test = mseLoss(predicted_epi, test_pheno_epi)\n",
    "    print(\"Epoch {}/{} done!\".format(epoch+1, epochs))\n",
    "    print(\"Training Loss: {}\".format(epoch_loss))\n",
    "    print(\"Testing Loss: {}\".format(loss_test.item()))\n",
    "    # early stopping\n",
    "    if loss_test.item() > epoch_loss + early_stop_tolerance:\n",
    "        early_stop_trigger_counter += 1\n",
    "        if early_stop_trigger_counter > 5:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_interaction_vars_tensor = torch.from_numpy(interaction_vars_np.astype(np.float32))\n",
    "model.eval()\n",
    "predicted_index = model.get_resilience_index(all_interaction_vars_tensor)\n",
    "all_resilience_index = predicted_index.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 10.,  16.,  34.,  54.,  70., 117., 142., 169., 196., 177., 209.,\n",
       "        261., 279., 305., 308., 256., 190., 143., 102.,  85.,  48.,  44.,\n",
       "         39.,  28.,  10.,  13.,   4.,   3.,   2.,   2.]),\n",
       " array([-0.08139557, -0.05348667, -0.02557777,  0.00233114,  0.03024004,\n",
       "         0.05814894,  0.08605785,  0.11396675,  0.14187565,  0.16978456,\n",
       "         0.19769347,  0.22560236,  0.25351128,  0.28142017,  0.30932906,\n",
       "         0.33723798,  0.36514688,  0.3930558 ,  0.42096469,  0.44887361,\n",
       "         0.4767825 ,  0.50469142,  0.53260028,  0.5605092 ,  0.58841813,\n",
       "         0.61632699,  0.64423591,  0.67214483,  0.70005375,  0.72796261,\n",
       "         0.75587153]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(all_resilience_index, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../checkpoints/Feb2_pm25_model_v3_nosp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
