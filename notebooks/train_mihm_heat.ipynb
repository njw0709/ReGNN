{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mihm.data.process import multi_cat_to_one_hot, binary_to_one_hot, standardize_continuous_cols, convert_categorical_to_ordinal\n",
    "from mihm.data.trainutils import train_test_split\n",
    "from mihm.model.mihm import MIHM, IndexPredictionModel\n",
    "from mihm.model.mihm_dataset import MIHMDataset\n",
    "from mihm.model.modelutils import get_index_prediction_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_cols = [\"zPCPhenoAge_acc\", \"m_HeatIndex_7d\", \"age2016\", \"female\", \"racethn\", \"eduy\", \"ihs_wealthf2016\", \"pmono\", \"PNK_pct\", \n",
    "            \"PBcell_pct\", \"PCD8_Plus_pct\", \"PCD4_Plus_pct\", \"PNCD8_Plus_pct\",\n",
    "            \"smoke2016\", \"drink2016\", \"bmi2016\", \"tractdis\", \"urban\", \"mar_cat2\", \"psyche2016\", \"stroke2016\", \"hibpe2016\",\n",
    "            \"diabe2016\", \"hearte2016\", \"ltactx2016\", \"mdactx2016\", \"vgactx2016\", \"chd2016\", \"dep2016\", \"adl2016\", \n",
    "            \"living2016\", \"division\"]\n",
    "\n",
    "\n",
    "df = pd.read_stata('../HeatResilience.dta', columns=read_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['female', 'racethn', 'urban', 'mar_cat2', \"psyche2016\", \"stroke2016\", \n",
    "                    \"hibpe2016\", \"diabe2016\", \"hearte2016\",  'living2016', 'division',]\n",
    "ordinal_cols = [\"smoke2016\",  'drink2016', 'ltactx2016', 'mdactx2016', 'vgactx2016']\n",
    "continuous_cols = ['eduy', 'ihs_wealthf2016', 'age2016', 'pmono','bmi2016', 'tractdis', 'chd2016', 'dep2016', 'adl2016', \"m_HeatIndex_7d\",\n",
    "                    \"PNK_pct\", \"PBcell_pct\", \"PCD8_Plus_pct\", \"PCD4_Plus_pct\", \"PNCD8_Plus_pct\",]\n",
    "for c in categorical_cols:\n",
    "    df[c] = df[c].astype(\"category\")\n",
    "# categorical = [c for c in df.columns if df[c].dtype == \"category\"]\n",
    "# separate binary vs multicategory cols\n",
    "binary_cats = [c for c in categorical_cols if df[c].nunique() <=2]\n",
    "multi_cats = [c for c in categorical_cols if df[c].nunique() > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess df for model\n",
    "df = binary_to_one_hot(df, binary_cats) # convert binary to one hot\n",
    "df = multi_cat_to_one_hot(df, multi_cats) # convert multi cat to one hot\n",
    "df = convert_categorical_to_ordinal(df, ordinal_cols) # convert ordinal to ordinal\n",
    "df_norm, mean_std_dict = standardize_continuous_cols(df, continuous_cols+ordinal_cols) # standardize continuous cols\n",
    "df_norm.dropna(inplace=True) # drop Nan rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zPCPhenoAge_acc', 'm_HeatIndex_7d', 'age2016', 'female', 'eduy',\n",
       "       'ihs_wealthf2016', 'pmono', 'PNK_pct', 'PBcell_pct', 'PCD8_Plus_pct',\n",
       "       'PCD4_Plus_pct', 'PNCD8_Plus_pct', 'smoke2016', 'drink2016', 'bmi2016',\n",
       "       'tractdis', 'mar_cat2', 'psyche2016', 'stroke2016', 'hibpe2016',\n",
       "       'diabe2016', 'hearte2016', 'ltactx2016', 'mdactx2016', 'vgactx2016',\n",
       "       'chd2016', 'dep2016', 'adl2016', 'living2016', 'racethn_0. NHW',\n",
       "       'racethn_1. NHB', 'racethn_2. Hispanic', 'racethn_3. Others',\n",
       "       'urban_1. urban', 'urban_2. suurban (code 2)', 'urban_3. ex-urban',\n",
       "       'division_Northeast', 'division_Midwest', 'division_South',\n",
       "       'division_West'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['female', 'eduy', 'ihs_wealthf2016', 'pmono', 'bmi2016', \"age2016\",\n",
    "            'tractdis', 'mar_cat2', 'psyche2016', 'stroke2016', 'hibpe2016', \n",
    "            'diabe2016', 'hearte2016', 'chd2016', 'dep2016', 'adl2016', 'living2016', \n",
    "            'smoke2016', 'drink2016', 'ltactx2016', 'mdactx2016', 'vgactx2016', # ordinals\n",
    "            'racethn_0. NHW', 'racethn_1. NHB', 'racethn_2. Hispanic', 'racethn_3. Others', # multi cats\n",
    "            'urban_1. urban', 'urban_2. suurban (code 2)', 'urban_3. ex-urban', \n",
    "            'division_Northeast', 'division_Midwest', 'division_South', 'division_West',\n",
    "            \"PNK_pct\", \"PBcell_pct\", \"PCD8_Plus_pct\", \"PCD4_Plus_pct\", \"PNCD8_Plus_pct\",]\n",
    "controlled_cols = [\n",
    "    \"m_HeatIndex_7d\",\n",
    "    \"pmono\",\n",
    "    \"PNK_pct\",\n",
    "    \"PBcell_pct\",\n",
    "    \"PCD8_Plus_pct\",\n",
    "    \"PCD4_Plus_pct\",\n",
    "    \"PNCD8_Plus_pct\",\n",
    "]\n",
    "interaction_predictors = [\n",
    "    \"female\", \"racethn_0. NHW\", \"racethn_1. NHB\", \"racethn_2. Hispanic\", \"racethn_3. Others\",\n",
    "    'eduy', 'ihs_wealthf2016', 'bmi2016', \n",
    "    'tractdis', 'mar_cat2', 'psyche2016', 'stroke2016', 'hibpe2016', \n",
    "    'diabe2016', 'hearte2016', 'chd2016', 'dep2016', 'adl2016', 'living2016', \n",
    "    'smoke2016', 'drink2016', 'ltactx2016', 'mdactx2016', 'vgactx2016',\n",
    "    'urban_1. urban', 'urban_2. suurban (code 2)', 'urban_3. ex-urban', \n",
    "    'division_Northeast', 'division_Midwest', 'division_South', 'division_West',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactor\n",
    "heat_cont_np = df_norm[\"m_HeatIndex_7d\"].to_numpy()\n",
    "# controlled vars\n",
    "controlled_vars_np = df_norm[controlled_cols].to_numpy()\n",
    "# interaction input vars\n",
    "interaction_vars_np = df_norm[interaction_predictors].to_numpy()\n",
    "# dependent var (label)\n",
    "pheno_epi_np = df_norm[\"zPCPhenoAge_acc\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of data points: 3325\n"
     ]
    }
   ],
   "source": [
    "num_elems, _ = controlled_vars_np.shape\n",
    "print(\"number of data points: {}\".format(num_elems))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train and test\n",
    "train_idx, test_idx = train_test_split(num_elems, 0.7)\n",
    "train_heat_cont = heat_cont_np[train_idx]\n",
    "train_controlled_vars = controlled_vars_np[train_idx]\n",
    "train_interaction_vars = interaction_vars_np[train_idx]\n",
    "train_pheno_epi = pheno_epi_np[train_idx]\n",
    "\n",
    "test_heat_cont = torch.from_numpy(heat_cont_np[test_idx].astype(np.float32))\n",
    "test_controlled_vars = torch.from_numpy(controlled_vars_np[test_idx].astype(np.float32))\n",
    "test_interaction_vars = torch.from_numpy(interaction_vars_np[test_idx].astype(np.float32))\n",
    "test_pheno_epi = torch.from_numpy(pheno_epi_np[test_idx].astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "train_dataset = MIHMDataset(train_heat_cont, train_controlled_vars, train_interaction_vars, train_pheno_epi)\n",
    "dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_var_size = interaction_vars_np.shape[1]\n",
    "controlled_var_size = controlled_vars_np.shape[1]\n",
    "hidden_layer_sizes = [50, 10, 1]\n",
    "model = MIHM(interaction_var_size, controlled_var_size, hidden_layer_sizes, include_interactor_bias=True, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "mseLoss = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003, weight_decay=0.1)\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namj/miniconda3/envs/hrs/lib/python3.9/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([998])) that is different to the input size (torch.Size([998, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Loss: 3.6031479835510254\n",
      "Epoch 1/300 done!\n",
      "Training Loss: 4.017033080259959\n",
      "Testing Loss: 3.3197600841522217\n",
      "Epoch 2/300 done!\n",
      "Training Loss: 3.602308770020803\n",
      "Testing Loss: 3.1333751678466797\n",
      "Epoch 3/300 done!\n",
      "Training Loss: 3.3526893655459085\n",
      "Testing Loss: 2.998250961303711\n",
      "Epoch 4/300 done!\n",
      "Training Loss: 3.1006683309872947\n",
      "Testing Loss: 2.894667148590088\n",
      "Epoch 5/300 done!\n",
      "Training Loss: 3.098994553089142\n",
      "Testing Loss: 2.8051180839538574\n",
      "Epoch 6/300 done!\n",
      "Training Loss: 2.971968799829483\n",
      "Testing Loss: 2.729034185409546\n",
      "Epoch 7/300 done!\n",
      "Training Loss: 2.826191579302152\n",
      "Testing Loss: 2.6570520401000977\n",
      "Epoch 8/300 done!\n",
      "Training Loss: 2.6837386737267175\n",
      "Testing Loss: 2.59257173538208\n",
      "Epoch 9/300 done!\n",
      "Training Loss: 2.609037463863691\n",
      "Testing Loss: 2.534175395965576\n",
      "Epoch 10/300 done!\n",
      "Training Loss: 2.5316686183214188\n",
      "Testing Loss: 2.478219985961914\n",
      "Epoch 11/300 done!\n",
      "Training Loss: 2.45475339392821\n",
      "Testing Loss: 2.426623821258545\n",
      "Epoch 12/300 done!\n",
      "Training Loss: 2.347423697511355\n",
      "Testing Loss: 2.379415988922119\n",
      "Epoch 13/300 done!\n",
      "Training Loss: 2.4136300732692084\n",
      "Testing Loss: 2.3309755325317383\n",
      "Epoch 14/300 done!\n",
      "Training Loss: 2.240084782242775\n",
      "Testing Loss: 2.2861177921295166\n",
      "Epoch 15/300 done!\n",
      "Training Loss: 2.2425044228633246\n",
      "Testing Loss: 2.2446999549865723\n",
      "Epoch 16/300 done!\n",
      "Training Loss: 2.209850162267685\n",
      "Testing Loss: 2.2064390182495117\n",
      "Epoch 17/300 done!\n",
      "Training Loss: 2.121496871113777\n",
      "Testing Loss: 2.1674115657806396\n",
      "Epoch 18/300 done!\n",
      "Training Loss: 2.058832804361979\n",
      "Testing Loss: 2.1297457218170166\n",
      "Epoch 19/300 done!\n",
      "Training Loss: 2.031087296704451\n",
      "Testing Loss: 2.0952513217926025\n",
      "Epoch 20/300 done!\n",
      "Training Loss: 1.994009827574094\n",
      "Testing Loss: 2.0615782737731934\n",
      "Epoch 21/300 done!\n",
      "Training Loss: 1.9901992777983348\n",
      "Testing Loss: 2.0287363529205322\n",
      "Epoch 22/300 done!\n",
      "Training Loss: 1.909310296177864\n",
      "Testing Loss: 1.9976848363876343\n",
      "Epoch 23/300 done!\n",
      "Training Loss: 1.9103748401006062\n",
      "Testing Loss: 1.9665497541427612\n",
      "Epoch 24/300 done!\n",
      "Training Loss: 1.8932957102855046\n",
      "Testing Loss: 1.936962604522705\n",
      "Epoch 25/300 done!\n",
      "Training Loss: 1.8202528258164723\n",
      "Testing Loss: 1.908577561378479\n",
      "Epoch 26/300 done!\n",
      "Training Loss: 1.825163796544075\n",
      "Testing Loss: 1.8816105127334595\n",
      "Epoch 27/300 done!\n",
      "Training Loss: 1.874603768189748\n",
      "Testing Loss: 1.856071949005127\n",
      "Epoch 28/300 done!\n",
      "Training Loss: 1.7283568282922108\n",
      "Testing Loss: 1.8297560214996338\n",
      "Epoch 29/300 done!\n",
      "Training Loss: 1.7297638257344563\n",
      "Testing Loss: 1.8066551685333252\n",
      "Epoch 30/300 done!\n",
      "Training Loss: 1.7102087636788685\n",
      "Testing Loss: 1.7835943698883057\n",
      "Epoch 31/300 done!\n",
      "Training Loss: 1.6972318142652512\n",
      "Testing Loss: 1.7614405155181885\n",
      "Epoch 32/300 done!\n",
      "Training Loss: 1.6530796587467194\n",
      "Testing Loss: 1.7389379739761353\n",
      "Epoch 33/300 done!\n",
      "Training Loss: 1.6564798553784688\n",
      "Testing Loss: 1.718695878982544\n",
      "Epoch 34/300 done!\n",
      "Training Loss: 1.607425034046173\n",
      "Testing Loss: 1.698466420173645\n",
      "Epoch 35/300 done!\n",
      "Training Loss: 1.5777977779507637\n",
      "Testing Loss: 1.6796696186065674\n",
      "Epoch 36/300 done!\n",
      "Training Loss: 1.6345198502143223\n",
      "Testing Loss: 1.6616922616958618\n",
      "Epoch 37/300 done!\n",
      "Training Loss: 1.5900574301679928\n",
      "Testing Loss: 1.6422332525253296\n",
      "Epoch 38/300 done!\n",
      "Training Loss: 1.5372001032034557\n",
      "Testing Loss: 1.6248093843460083\n",
      "Epoch 39/300 done!\n",
      "Training Loss: 1.5520324011643727\n",
      "Testing Loss: 1.609218716621399\n",
      "Epoch 40/300 done!\n",
      "Training Loss: 1.5699452857176464\n",
      "Testing Loss: 1.5939825773239136\n",
      "Epoch 41/300 done!\n",
      "Training Loss: 1.498763531446457\n",
      "Testing Loss: 1.5776900053024292\n",
      "Epoch 42/300 done!\n",
      "Training Loss: 1.4895899519324303\n",
      "Testing Loss: 1.562024474143982\n",
      "Epoch 43/300 done!\n",
      "Training Loss: 1.4699955383936565\n",
      "Testing Loss: 1.5475836992263794\n",
      "Epoch 44/300 done!\n",
      "Training Loss: 1.4125510503848393\n",
      "Testing Loss: 1.5334019660949707\n",
      "Epoch 45/300 done!\n",
      "Training Loss: 1.4072308093309402\n",
      "Testing Loss: 1.5204570293426514\n",
      "Epoch 46/300 done!\n",
      "Training Loss: 1.4495065783460934\n",
      "Testing Loss: 1.5065702199935913\n",
      "Epoch 47/300 done!\n",
      "Training Loss: 1.4255388031403224\n",
      "Testing Loss: 1.493459701538086\n",
      "Epoch 48/300 done!\n",
      "Training Loss: 1.4153684477011363\n",
      "Testing Loss: 1.4814451932907104\n",
      "Epoch 49/300 done!\n",
      "Training Loss: 1.3490176126360893\n",
      "Testing Loss: 1.4705102443695068\n",
      "Epoch 50/300 done!\n",
      "Training Loss: 1.3427918329834938\n",
      "Testing Loss: 1.459522008895874\n",
      "Epoch 51/300 done!\n",
      "Training Loss: 1.3640706688165665\n",
      "Testing Loss: 1.4483528137207031\n",
      "Epoch 52/300 done!\n",
      "Training Loss: 1.3303573057055473\n",
      "Testing Loss: 1.4378752708435059\n",
      "Epoch 53/300 done!\n",
      "Training Loss: 1.3711701383193333\n",
      "Testing Loss: 1.4270704984664917\n",
      "Epoch 54/300 done!\n",
      "Training Loss: 1.2963880846897762\n",
      "Testing Loss: 1.4168856143951416\n",
      "Epoch 55/300 done!\n",
      "Training Loss: 1.3022635256250699\n",
      "Testing Loss: 1.4067076444625854\n",
      "Epoch 56/300 done!\n",
      "Training Loss: 1.3082576791445415\n",
      "Testing Loss: 1.3961271047592163\n",
      "Epoch 57/300 done!\n",
      "Training Loss: 1.2969374010960262\n",
      "Testing Loss: 1.3866195678710938\n",
      "Epoch 58/300 done!\n",
      "Training Loss: 1.2832975039879482\n",
      "Testing Loss: 1.376817226409912\n",
      "Epoch 59/300 done!\n",
      "Training Loss: 1.2449434598286946\n",
      "Testing Loss: 1.367385983467102\n",
      "Epoch 60/300 done!\n",
      "Training Loss: 1.2497015073895454\n",
      "Testing Loss: 1.359376311302185\n",
      "Epoch 61/300 done!\n",
      "Training Loss: 1.2465699091553688\n",
      "Testing Loss: 1.351124882698059\n",
      "Epoch 62/300 done!\n",
      "Training Loss: 1.2534453844030697\n",
      "Testing Loss: 1.3420311212539673\n",
      "Epoch 63/300 done!\n",
      "Training Loss: 1.2303285722931225\n",
      "Testing Loss: 1.3337591886520386\n",
      "Epoch 64/300 done!\n",
      "Training Loss: 1.201451023419698\n",
      "Testing Loss: 1.3263112306594849\n",
      "Epoch 65/300 done!\n",
      "Training Loss: 1.2075243666768074\n",
      "Testing Loss: 1.318799614906311\n",
      "Epoch 66/300 done!\n",
      "Training Loss: 1.2175192063053448\n",
      "Testing Loss: 1.311415672302246\n",
      "Epoch 67/300 done!\n",
      "Training Loss: 1.1939692236483097\n",
      "Testing Loss: 1.3033078908920288\n",
      "Epoch 68/300 done!\n",
      "Training Loss: 1.19406658411026\n",
      "Testing Loss: 1.2957260608673096\n",
      "Epoch 69/300 done!\n",
      "Training Loss: 1.184994600713253\n",
      "Testing Loss: 1.288501501083374\n",
      "Epoch 70/300 done!\n",
      "Training Loss: 1.170219751695792\n",
      "Testing Loss: 1.2814050912857056\n",
      "Epoch 71/300 done!\n",
      "Training Loss: 1.1779518152276676\n",
      "Testing Loss: 1.2740733623504639\n",
      "Epoch 72/300 done!\n",
      "Training Loss: 1.1829082742333412\n",
      "Testing Loss: 1.26774001121521\n",
      "Epoch 73/300 done!\n",
      "Training Loss: 1.1633346031109493\n",
      "Testing Loss: 1.2614246606826782\n",
      "Epoch 74/300 done!\n",
      "Training Loss: 1.1752777323126793\n",
      "Testing Loss: 1.2555336952209473\n",
      "Epoch 75/300 done!\n",
      "Training Loss: 1.1502529978752136\n",
      "Testing Loss: 1.2490118741989136\n",
      "Epoch 76/300 done!\n",
      "Training Loss: 1.1284007628758748\n",
      "Testing Loss: 1.243535041809082\n",
      "Epoch 77/300 done!\n",
      "Training Loss: 1.145927796761195\n",
      "Testing Loss: 1.2376458644866943\n",
      "Epoch 78/300 done!\n",
      "Training Loss: 1.1493269130587578\n",
      "Testing Loss: 1.2325992584228516\n",
      "Epoch 79/300 done!\n",
      "Training Loss: 1.13796433309714\n",
      "Testing Loss: 1.2269985675811768\n",
      "Epoch 80/300 done!\n",
      "Training Loss: 1.1236180911461513\n",
      "Testing Loss: 1.2211809158325195\n",
      "Epoch 81/300 done!\n",
      "Training Loss: 1.1294705246885617\n",
      "Testing Loss: 1.215091586112976\n",
      "Epoch 82/300 done!\n",
      "Training Loss: 1.1248833611607552\n",
      "Testing Loss: 1.210361123085022\n",
      "Epoch 83/300 done!\n",
      "Training Loss: 1.108472948273023\n",
      "Testing Loss: 1.205244779586792\n",
      "Epoch 84/300 done!\n",
      "Training Loss: 1.1069413324197133\n",
      "Testing Loss: 1.2003846168518066\n",
      "Epoch 85/300 done!\n",
      "Training Loss: 1.104531779885292\n",
      "Testing Loss: 1.1951655149459839\n",
      "Epoch 86/300 done!\n",
      "Training Loss: 1.091843234996001\n",
      "Testing Loss: 1.189821720123291\n",
      "Epoch 87/300 done!\n",
      "Training Loss: 1.1007024347782135\n",
      "Testing Loss: 1.1852951049804688\n",
      "Epoch 88/300 done!\n",
      "Training Loss: 1.088155133028825\n",
      "Testing Loss: 1.179911732673645\n",
      "Epoch 89/300 done!\n",
      "Training Loss: 1.0713253567616146\n",
      "Testing Loss: 1.1760683059692383\n",
      "Epoch 90/300 done!\n",
      "Training Loss: 1.0707044328252475\n",
      "Testing Loss: 1.172211766242981\n",
      "Epoch 91/300 done!\n",
      "Training Loss: 1.069784052670002\n",
      "Testing Loss: 1.1675338745117188\n",
      "Epoch 92/300 done!\n",
      "Training Loss: 1.0828446994225185\n",
      "Testing Loss: 1.163858413696289\n",
      "Epoch 93/300 done!\n",
      "Training Loss: 1.0682488158345222\n",
      "Testing Loss: 1.1594125032424927\n",
      "Epoch 94/300 done!\n",
      "Training Loss: 1.0507459466656048\n",
      "Testing Loss: 1.1552822589874268\n",
      "Epoch 95/300 done!\n",
      "Training Loss: 1.0535033692916234\n",
      "Testing Loss: 1.1517199277877808\n",
      "Epoch 96/300 done!\n",
      "Training Loss: 1.0541135544578235\n",
      "Testing Loss: 1.148171305656433\n",
      "Epoch 97/300 done!\n",
      "Training Loss: 1.0467174351215363\n",
      "Testing Loss: 1.1438952684402466\n",
      "Epoch 98/300 done!\n",
      "Training Loss: 1.0811660215258598\n",
      "Testing Loss: 1.140702724456787\n",
      "Epoch 99/300 done!\n",
      "Training Loss: 1.0644426842530568\n",
      "Testing Loss: 1.137046217918396\n",
      "Epoch 100/300 done!\n",
      "Training Loss: 1.0380609557032585\n",
      "Testing Loss: 1.1341649293899536\n",
      "Epoch 101/300 done!\n",
      "Training Loss: 1.0251665115356445\n",
      "Testing Loss: 1.131321668624878\n",
      "Epoch 102/300 done!\n",
      "Training Loss: 1.0212498257557552\n",
      "Testing Loss: 1.1287034749984741\n",
      "Epoch 103/300 done!\n",
      "Training Loss: 1.0196981703241665\n",
      "Testing Loss: 1.1261416673660278\n",
      "Epoch 104/300 done!\n",
      "Training Loss: 1.0247090458869934\n",
      "Testing Loss: 1.1232082843780518\n",
      "Epoch 105/300 done!\n",
      "Training Loss: 1.0582619334260623\n",
      "Testing Loss: 1.119744896888733\n",
      "Epoch 106/300 done!\n",
      "Training Loss: 1.038520373404026\n",
      "Testing Loss: 1.1165225505828857\n",
      "Epoch 107/300 done!\n",
      "Training Loss: 1.0118070791165035\n",
      "Testing Loss: 1.1145286560058594\n",
      "Epoch 108/300 done!\n",
      "Training Loss: 1.0206706499059994\n",
      "Testing Loss: 1.1116608381271362\n",
      "Epoch 109/300 done!\n",
      "Training Loss: 1.0250515937805176\n",
      "Testing Loss: 1.1090024709701538\n",
      "Epoch 110/300 done!\n",
      "Training Loss: 1.029869941373666\n",
      "Testing Loss: 1.1065763235092163\n",
      "Epoch 111/300 done!\n",
      "Training Loss: 1.0084226851662\n",
      "Testing Loss: 1.103928804397583\n",
      "Epoch 112/300 done!\n",
      "Training Loss: 1.0209361736973126\n",
      "Testing Loss: 1.1028028726577759\n",
      "Epoch 113/300 done!\n",
      "Training Loss: 1.0157240281502407\n",
      "Testing Loss: 1.1000633239746094\n",
      "Epoch 114/300 done!\n",
      "Training Loss: 1.0182255680362384\n",
      "Testing Loss: 1.0985816717147827\n",
      "Epoch 115/300 done!\n",
      "Training Loss: 0.9847068091233572\n",
      "Testing Loss: 1.0967782735824585\n",
      "Epoch 116/300 done!\n",
      "Training Loss: 1.0249557221929233\n",
      "Testing Loss: 1.0940710306167603\n",
      "Epoch 117/300 done!\n",
      "Training Loss: 0.9845084299643835\n",
      "Testing Loss: 1.0928040742874146\n",
      "Epoch 118/300 done!\n",
      "Training Loss: 1.0208399842182796\n",
      "Testing Loss: 1.0908108949661255\n",
      "Epoch 119/300 done!\n",
      "Training Loss: 0.9933939725160599\n",
      "Testing Loss: 1.0890575647354126\n",
      "Epoch 120/300 done!\n",
      "Training Loss: 0.9925193414092064\n",
      "Testing Loss: 1.086671233177185\n",
      "Epoch 121/300 done!\n",
      "Training Loss: 0.9836761206388474\n",
      "Testing Loss: 1.0849944353103638\n",
      "Epoch 122/300 done!\n",
      "Training Loss: 1.0402378638585408\n",
      "Testing Loss: 1.0829758644104004\n",
      "Epoch 123/300 done!\n",
      "Training Loss: 0.9985979745785395\n",
      "Testing Loss: 1.080771803855896\n",
      "Epoch 124/300 done!\n",
      "Training Loss: 1.0236620058616002\n",
      "Testing Loss: 1.0795958042144775\n",
      "Epoch 125/300 done!\n",
      "Training Loss: 0.9914273321628571\n",
      "Testing Loss: 1.0770514011383057\n",
      "Epoch 126/300 done!\n",
      "Training Loss: 0.9656437685092291\n",
      "Testing Loss: 1.0755349397659302\n",
      "Epoch 127/300 done!\n",
      "Training Loss: 0.9787443826595942\n",
      "Testing Loss: 1.0746484994888306\n",
      "Epoch 128/300 done!\n",
      "Training Loss: 0.9745893503228823\n",
      "Testing Loss: 1.0733585357666016\n",
      "Epoch 129/300 done!\n",
      "Training Loss: 0.9994087542096773\n",
      "Testing Loss: 1.0722359418869019\n",
      "Epoch 130/300 done!\n",
      "Training Loss: 0.969403458138307\n",
      "Testing Loss: 1.069714069366455\n",
      "Epoch 131/300 done!\n",
      "Training Loss: 1.0004029323657353\n",
      "Testing Loss: 1.0675339698791504\n",
      "Epoch 132/300 done!\n",
      "Training Loss: 0.9707266887029012\n",
      "Testing Loss: 1.0671029090881348\n",
      "Epoch 133/300 done!\n",
      "Training Loss: 0.9556625237067541\n",
      "Testing Loss: 1.0669511556625366\n",
      "Epoch 134/300 done!\n",
      "Training Loss: 0.9814887071649233\n",
      "Testing Loss: 1.066185712814331\n",
      "Epoch 135/300 done!\n",
      "Training Loss: 0.9719133079051971\n",
      "Testing Loss: 1.0647584199905396\n",
      "Epoch 136/300 done!\n",
      "Training Loss: 0.9551374986767769\n",
      "Testing Loss: 1.0639299154281616\n",
      "Epoch 137/300 done!\n",
      "Training Loss: 0.9614902486403784\n",
      "Testing Loss: 1.0633612871170044\n",
      "Epoch 138/300 done!\n",
      "Training Loss: 0.9674420952796936\n",
      "Testing Loss: 1.0629509687423706\n",
      "Epoch 139/300 done!\n",
      "Training Loss: 0.9699400092164675\n",
      "Testing Loss: 1.061710000038147\n",
      "Epoch 140/300 done!\n",
      "Training Loss: 0.9470687086383501\n",
      "Testing Loss: 1.0615065097808838\n",
      "Epoch 141/300 done!\n",
      "Training Loss: 0.9693158666292826\n",
      "Testing Loss: 1.0601539611816406\n",
      "Epoch 142/300 done!\n",
      "Training Loss: 0.9680225228269895\n",
      "Testing Loss: 1.059903621673584\n",
      "Epoch 143/300 done!\n",
      "Training Loss: 0.96243666857481\n",
      "Testing Loss: 1.0582873821258545\n",
      "Epoch 144/300 done!\n",
      "Training Loss: 0.9373260339101156\n",
      "Testing Loss: 1.057915449142456\n",
      "Epoch 145/300 done!\n",
      "Training Loss: 0.9684434061249098\n",
      "Testing Loss: 1.0584362745285034\n",
      "Epoch 146/300 done!\n",
      "Training Loss: 0.9557153234879175\n",
      "Testing Loss: 1.0566242933273315\n",
      "Epoch 147/300 done!\n",
      "Training Loss: 0.947869248688221\n",
      "Testing Loss: 1.0557844638824463\n",
      "Epoch 148/300 done!\n",
      "Training Loss: 0.9662337501843771\n",
      "Testing Loss: 1.0549534559249878\n",
      "Epoch 149/300 done!\n",
      "Training Loss: 0.9377855211496353\n",
      "Testing Loss: 1.0541532039642334\n",
      "Epoch 150/300 done!\n",
      "Training Loss: 0.9435186634461085\n",
      "Testing Loss: 1.0540688037872314\n",
      "Epoch 151/300 done!\n",
      "Training Loss: 0.9517755185564359\n",
      "Testing Loss: 1.0527149438858032\n",
      "Epoch 152/300 done!\n",
      "Training Loss: 0.9601326485474905\n",
      "Testing Loss: 1.0521957874298096\n",
      "Epoch 153/300 done!\n",
      "Training Loss: 0.9456808542211851\n",
      "Testing Loss: 1.0513384342193604\n",
      "Epoch 154/300 done!\n",
      "Training Loss: 0.9413554891943932\n",
      "Testing Loss: 1.0520756244659424\n",
      "Epoch 155/300 done!\n",
      "Training Loss: 0.9318157558639845\n",
      "Testing Loss: 1.0532114505767822\n",
      "Epoch 156/300 done!\n",
      "Training Loss: 0.9522119909524918\n",
      "Testing Loss: 1.053032636642456\n",
      "Epoch 157/300 done!\n",
      "Training Loss: 0.9292159453034401\n",
      "Testing Loss: 1.0521975755691528\n",
      "Epoch 158/300 done!\n",
      "Training Loss: 0.9656487355629603\n",
      "Testing Loss: 1.0518646240234375\n",
      "Epoch 159/300 done!\n",
      "Training Loss: 0.9601225952307383\n",
      "Testing Loss: 1.0524468421936035\n",
      "Epoch 160/300 done!\n",
      "Training Loss: 0.9386203487714132\n",
      "Testing Loss: 1.0514529943466187\n",
      "Epoch 161/300 done!\n",
      "Training Loss: 0.9477385232845942\n",
      "Testing Loss: 1.0501497983932495\n",
      "Epoch 162/300 done!\n",
      "Training Loss: 0.932809074719747\n",
      "Testing Loss: 1.0500390529632568\n",
      "Epoch 163/300 done!\n",
      "Training Loss: 0.9318762843807539\n",
      "Testing Loss: 1.0497912168502808\n",
      "Epoch 164/300 done!\n",
      "Training Loss: 0.93696757654349\n",
      "Testing Loss: 1.049469232559204\n",
      "Epoch 165/300 done!\n",
      "Training Loss: 0.9300769219795862\n",
      "Testing Loss: 1.050160527229309\n",
      "Epoch 166/300 done!\n",
      "Training Loss: 0.9347949400544167\n",
      "Testing Loss: 1.0510799884796143\n",
      "Epoch 167/300 done!\n",
      "Training Loss: 0.9340071653326353\n",
      "Testing Loss: 1.0524441003799438\n",
      "Epoch 168/300 done!\n",
      "Training Loss: 0.9159728661179543\n",
      "Testing Loss: 1.053126573562622\n",
      "Epoch 169/300 done!\n",
      "Training Loss: 0.9202242443958918\n",
      "Testing Loss: 1.0547670125961304\n",
      "Epoch 170/300 done!\n",
      "Training Loss: 0.9193057517210642\n",
      "Testing Loss: 1.0549157857894897\n",
      "Epoch 171/300 done!\n",
      "Training Loss: 0.9209701096018156\n",
      "Testing Loss: 1.0542532205581665\n",
      "Epoch 172/300 done!\n",
      "Training Loss: 0.9201852455735207\n",
      "Testing Loss: 1.0549505949020386\n",
      "Epoch 173/300 done!\n",
      "Training Loss: 0.9298715069890022\n",
      "Testing Loss: 1.0548532009124756\n",
      "Epoch 174/300 done!\n",
      "Training Loss: 0.9120385323961576\n",
      "Testing Loss: 1.055390477180481\n",
      "Epoch 175/300 done!\n",
      "Training Loss: 0.928923080364863\n",
      "Testing Loss: 1.054988145828247\n",
      "Epoch 176/300 done!\n",
      "Training Loss: 0.9073549223442873\n",
      "Testing Loss: 1.054608702659607\n",
      "Epoch 177/300 done!\n",
      "Training Loss: 0.9393457596500715\n",
      "Testing Loss: 1.0561171770095825\n",
      "Epoch 178/300 done!\n",
      "Training Loss: 0.8966588651140531\n",
      "Testing Loss: 1.05693781375885\n",
      "Epoch 179/300 done!\n",
      "Training Loss: 0.9111932640274366\n",
      "Testing Loss: 1.0554252862930298\n",
      "Epoch 180/300 done!\n",
      "Training Loss: 0.9087265084187189\n",
      "Testing Loss: 1.0575133562088013\n",
      "Epoch 181/300 done!\n",
      "Training Loss: 0.9074041123191515\n",
      "Testing Loss: 1.0582244396209717\n",
      "Epoch 182/300 done!\n",
      "Training Loss: 0.9119851614038149\n",
      "Testing Loss: 1.0575871467590332\n",
      "Epoch 183/300 done!\n",
      "Training Loss: 0.901524489124616\n",
      "Testing Loss: 1.0581393241882324\n",
      "Epoch 184/300 done!\n",
      "Training Loss: 0.9231651897231737\n",
      "Testing Loss: 1.060251235961914\n",
      "Epoch 185/300 done!\n",
      "Training Loss: 0.9012097989519438\n",
      "Testing Loss: 1.0572428703308105\n",
      "Epoch 186/300 done!\n",
      "Training Loss: 0.8954251892864704\n",
      "Testing Loss: 1.057028889656067\n",
      "Epoch 187/300 done!\n",
      "Training Loss: 0.9005715548992157\n",
      "Testing Loss: 1.0575133562088013\n",
      "Epoch 188/300 done!\n",
      "Training Loss: 0.9110301782687505\n",
      "Testing Loss: 1.0590229034423828\n",
      "Epoch 189/300 done!\n",
      "Training Loss: 0.8930718824267387\n",
      "Testing Loss: 1.0585157871246338\n",
      "Epoch 190/300 done!\n",
      "Training Loss: 0.898053747912248\n",
      "Testing Loss: 1.0585063695907593\n",
      "Epoch 191/300 done!\n",
      "Training Loss: 0.8882234146197637\n",
      "Testing Loss: 1.0595048666000366\n",
      "Epoch 192/300 done!\n",
      "Training Loss: 0.8985168809692065\n",
      "Testing Loss: 1.0598942041397095\n",
      "Epoch 193/300 done!\n",
      "Training Loss: 0.8959125205874443\n",
      "Testing Loss: 1.0601304769515991\n",
      "Epoch 194/300 done!\n",
      "Training Loss: 0.8794384797414144\n",
      "Testing Loss: 1.0611751079559326\n",
      "Epoch 195/300 done!\n",
      "Training Loss: 0.8817400907476743\n",
      "Testing Loss: 1.0639053583145142\n",
      "Epoch 196/300 done!\n",
      "Training Loss: 0.9002592439452807\n",
      "Testing Loss: 1.0638411045074463\n",
      "Epoch 197/300 done!\n",
      "Training Loss: 0.8917561893661817\n",
      "Testing Loss: 1.0633070468902588\n",
      "Epoch 198/300 done!\n",
      "Training Loss: 0.8808133254448572\n",
      "Testing Loss: 1.0634725093841553\n",
      "Epoch 199/300 done!\n",
      "Training Loss: 0.8752102851867676\n",
      "Testing Loss: 1.0654925107955933\n",
      "Epoch 200/300 done!\n",
      "Training Loss: 0.8688087239861488\n",
      "Testing Loss: 1.0655347108840942\n",
      "Epoch 201/300 done!\n",
      "Training Loss: 0.8898679514726003\n",
      "Testing Loss: 1.0675183534622192\n",
      "Epoch 202/300 done!\n",
      "Training Loss: 0.8903408770759901\n",
      "Testing Loss: 1.065104365348816\n",
      "Epoch 203/300 done!\n",
      "Training Loss: 0.8893900215625763\n",
      "Testing Loss: 1.0656052827835083\n",
      "Epoch 204/300 done!\n",
      "Training Loss: 0.9059908737738928\n",
      "Testing Loss: 1.0630933046340942\n",
      "Epoch 205/300 done!\n",
      "Training Loss: 0.885218101243178\n",
      "Testing Loss: 1.0654597282409668\n",
      "Epoch 206/300 done!\n",
      "Training Loss: 0.8843964835007986\n",
      "Testing Loss: 1.0644100904464722\n",
      "Epoch 207/300 done!\n",
      "Training Loss: 0.883120117088159\n",
      "Testing Loss: 1.0641900300979614\n",
      "Epoch 208/300 done!\n",
      "Training Loss: 0.8766612559556961\n",
      "Testing Loss: 1.0658764839172363\n",
      "Epoch 209/300 done!\n",
      "Training Loss: 0.8920555959145228\n",
      "Testing Loss: 1.0667648315429688\n",
      "Epoch 210/300 done!\n",
      "Training Loss: 0.9138834128777186\n",
      "Testing Loss: 1.0676155090332031\n",
      "Epoch 211/300 done!\n",
      "Training Loss: 0.8901924540599188\n",
      "Testing Loss: 1.0677025318145752\n",
      "Epoch 212/300 done!\n",
      "Training Loss: 0.875112903614839\n",
      "Testing Loss: 1.0682008266448975\n",
      "Epoch 213/300 done!\n",
      "Training Loss: 0.8808951713144779\n",
      "Testing Loss: 1.0695815086364746\n",
      "Epoch 214/300 done!\n",
      "Training Loss: 0.8665737584233284\n",
      "Testing Loss: 1.0725167989730835\n",
      "Epoch 215/300 done!\n",
      "Training Loss: 0.8647250893215338\n",
      "Testing Loss: 1.0735015869140625\n",
      "Epoch 216/300 done!\n",
      "Training Loss: 0.895256407558918\n",
      "Testing Loss: 1.072689175605774\n",
      "Epoch 217/300 done!\n",
      "Training Loss: 0.8501908493538698\n",
      "Testing Loss: 1.075280785560608\n",
      "Epoch 218/300 done!\n",
      "Training Loss: 0.882076824704806\n",
      "Testing Loss: 1.074479579925537\n",
      "Epoch 219/300 done!\n",
      "Training Loss: 0.8559560353557268\n",
      "Testing Loss: 1.0735597610473633\n",
      "Epoch 220/300 done!\n",
      "Training Loss: 0.8673765137791634\n",
      "Testing Loss: 1.0747382640838623\n",
      "Epoch 221/300 done!\n",
      "Training Loss: 0.8875009839733442\n",
      "Testing Loss: 1.0768895149230957\n",
      "Epoch 222/300 done!\n",
      "Training Loss: 0.8595274810989698\n",
      "Testing Loss: 1.0756295919418335\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predicted_epi, predicted_index = model(test_interaction_vars, test_heat_cont, test_controlled_vars)\n",
    "    loss_test = mseLoss(predicted_epi, test_pheno_epi)\n",
    "    print(\"Testing Loss: {}\".format(loss_test.item()))\n",
    "early_stop_trigger_counter = 0\n",
    "early_stop_tolerance = 0.2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, sample in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        predicted_epi, predicted_index = model(sample[\"interaction_input_vars\"], sample[\"interactor_var\"], sample[\"controlled_vars\"])\n",
    "        label = torch.unsqueeze(sample[\"label\"], 1)\n",
    "        loss = mseLoss(predicted_epi, label)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # print average loss for epoch\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "\n",
    "    # evaluation on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_epi, predicted_interaction = model(test_interaction_vars, test_heat_cont, test_controlled_vars)\n",
    "        loss_test = mseLoss(predicted_epi, test_pheno_epi)\n",
    "    print(\"Epoch {}/{} done!\".format(epoch+1, epochs))\n",
    "    print(\"Training Loss: {}\".format(epoch_loss))\n",
    "    print(\"Testing Loss: {}\".format(loss_test.item()))\n",
    "    # early stopping\n",
    "    if loss_test.item() > epoch_loss + early_stop_tolerance:\n",
    "        early_stop_trigger_counter += 1\n",
    "        if early_stop_trigger_counter > 5:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_interaction_vars_tensor = torch.from_numpy(interaction_vars_np.astype(np.float32))\n",
    "model.eval()\n",
    "predicted_index = model.get_resilience_index(all_interaction_vars_tensor)\n",
    "all_resilience_index = predicted_index.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 72., 145., 188., 186., 211., 178., 206., 186., 195., 161., 140.,\n",
       "        162., 167., 143., 127., 120.,  99., 116.,  86., 103.,  78.,  59.,\n",
       "         56.,  46.,  37.,  30.,  17.,   6.,   3.,   2.]),\n",
       " array([-0.89884245, -0.83920133, -0.77956015, -0.71991903, -0.66027784,\n",
       "        -0.60063672, -0.5409956 , -0.48135442, -0.42171329, -0.36207214,\n",
       "        -0.30243099, -0.24278983, -0.1831487 , -0.12350754, -0.0638664 ,\n",
       "        -0.00422525,  0.05541589,  0.11505704,  0.17469819,  0.23433933,\n",
       "         0.29398048,  0.35362163,  0.41326278,  0.47290391,  0.53254509,\n",
       "         0.59218621,  0.65182734,  0.71146852,  0.77110964,  0.83075082,\n",
       "         0.89039195]),\n",
       " <BarContainer object of 30 artists>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(all_resilience_index, bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../checkpoints/Feb2_heat_model_v3_nosp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
